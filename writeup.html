<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Vehicle Detection Project</title>
<!-- 2017-03-08 Wed 17:09 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="David A. Ventimiglia" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<style>@import 'https://fonts.googleapis.com/css?family=Quattrocento';</style>
<link rel="stylesheet" type="text/css" href="base.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Vehicle Detection Project</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
The goals / steps of this project are the following:
</p>

<ul class="org-ul">
<li>Perform a Histogram of Oriented Gradients (HOG) feature extraction
on a labeled training set of images and train a classifier Linear
SVM classifier
</li>
<li>Optionally, you can also apply a color transform and append binned
color features, as well as histograms of color, to your HOG
feature vector.
</li>
<li>Note: for those first two steps don't forget to normalize your
features and randomize a selection for training and testing.
</li>
<li>Implement a sliding-window technique and use your trained
classifier to search for vehicles in images.
</li>
<li>Run your pipeline on a video stream (start with the <code>test_video.mp4</code>
and later implement on full <code>project_video.mp4</code>) and create a heat
map of recurring detections frame by frame to reject outliers and
follow detected vehicles.
</li>
<li>Estimate a bounding box for vehicles detected.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Setup</h2>
<div class="outline-text-2" id="text-2">
<p>
The initial setup includes creating the <a href="https://www.python.org/">Python</a> environment with the
packages that the project needs and uses.
</p>

<p>
<b>NOTE I</b>: The code for this project can be found in the repository's
<a href="detect.py">detect.py</a> file.  <i>However</i>, all of the code in that file was
generated directly from the code blocks that appear in this file and
as such, contain no new information.  Reviewers can consult either
that Python file or this document.
</p>

<p>
<b>NOTE II</b>: This document is presented in a variety of formats.
There is this Emacs Org-Mode <a href="writeup.html">file</a>, a <a href="writeup.pdf">PDF</a> generated using <i>LaTeX</i>, an
<a href="writeup.html">HTML</a> file, and a <a href="writeup.md">Markdown</a> file.  The Markdown file will be rendered
directly by GitHub when viewed on the web.  The HTML version can be
rendered either by cloning the repository to your own computer and
opening the file in a browser locally.  Or, you can view the same
file in GitHub Pages at <a href="https://dventimi.github.io/CarND-Advanced-Lane-Lines/writeup.html">this link</a>.  It looks quite a bit better than
the GitHub-rendered Markdown version.
</p>

<dl class="org-dl">
<dt> <a href="http://matplotlib.org/">matplotlib</a> </dt><dd>plotting and image processing tools
</dd>
<dt> <a href="http://www.numpy.org/">NumPy</a> </dt><dd>foundational scientific computing library
</dd>
<dt> <a href="http://zulko.github.io/moviepy/">MoviePy</a> </dt><dd>video processing tools
</dd>
<dt> <a href="http://opencv.org/">OpenCV</a> </dt><dd>computer vision library
</dd>
</dl>

<p>
The <a href="https://github.com/">GitHub</a> <a href="https://github.com/dventimi/CarND-Advanced-Lane-Lines">repository</a> for this project contains an <a href="environment.yml">environment.yml</a>
file that can be used to create and activate a <a href="https://conda.io/docs/">Conda</a> environment
with these commands.
</p>

<div class="org-src-container">

<pre class="src src-sh">conda create --name CarND-Vehicle-Detection --clone CarND-Advanced-Lane-Lines
<span style="color: #e090d7;">source</span> activate CarND-Vehicle-Detection
conda env export &gt; environment.yml
</pre>
</div>

<div class="org-src-container">

<pre class="src src-sh">conda env create --file environment.yml --name CarND-Vehicle-Detection
<span style="color: #e090d7;">source</span> activate CarND-Vehicle-Detection
</pre>
</div>

<p>
Once activated this environment is used to launch Python in whatever
way one likes, such as a <a href="https://www.python.org/shell/">Python shell</a>, a <a href="https://ipython.org/">IPython shell</a>, or a <a href="http://jupyter.org/">jupyter
notebook</a>.  Having done that, the usual first step is to import the
packages that are used.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">from</span> glob <span style="color: #b4fa70;">import</span> glob
<span style="color: #b4fa70;">from</span> itertools <span style="color: #b4fa70;">import</span> groupby, islice, zip_longest, cycle, filterfalse, chain
<span style="color: #b4fa70;">from</span> moviepy.editor <span style="color: #b4fa70;">import</span> VideoFileClip, VideoClip
<span style="color: #b4fa70;">from</span> random <span style="color: #b4fa70;">import</span> choice, sample
<span style="color: #b4fa70;">from</span> scipy.ndimage.measurements <span style="color: #b4fa70;">import</span> label
<span style="color: #b4fa70;">from</span> skimage.feature <span style="color: #b4fa70;">import</span> hog
<span style="color: #b4fa70;">from</span> sklearn.model_selection <span style="color: #b4fa70;">import</span> train_test_split
<span style="color: #b4fa70;">from</span> sklearn.preprocessing <span style="color: #b4fa70;">import</span> StandardScaler
<span style="color: #b4fa70;">from</span> sklearn.svm <span style="color: #b4fa70;">import</span> LinearSVC
<span style="color: #b4fa70;">import</span> cv2
<span style="color: #b4fa70;">import</span> math
<span style="color: #b4fa70;">import</span> matplotlib.image <span style="color: #b4fa70;">as</span> mpimg
<span style="color: #b4fa70;">import</span> matplotlib.pyplot <span style="color: #b4fa70;">as</span> plt
<span style="color: #b4fa70;">import</span> numpy <span style="color: #b4fa70;">as</span> np
<span style="color: #b4fa70;">import</span> pdb
<span style="color: #b4fa70;">import</span> pickle
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Histogram of Oriented Gradients (HOG)</h2>
<div class="outline-text-2" id="text-3">
<p>
The first step in this project was to create a vehicle classifier
that was capable of identifying vehicles in an image, since we can
treat the frames of the video we are processing as individual
images.  Broadly, there are two main approaches for this task.  
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Neural Networks</a>
</li>
</ul>

<p>
Because the emphasis of this module in the Udacity course seemed to
focus on the computer vision approach, and because we already used
neural networks for two previous projects, I chose to explore the
first of these.
</p>

<p>
Though the two are similar (and have the same objective:
classification), one of the hallmarks of the computer vision
approach seems to be manual <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>: relying on human
experience to select useful features for a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> problem.
In the class, we explored several.
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Color_space">color space</a> selection
</li>
<li><a href="https://en.wikipedia.org/wiki/Data_binning">binning</a> of spatial features
</li>
<li><a href="https://en.wikipedia.org/wiki/Color_histogram">color histograms</a>
</li>
<li><a href="http://www.learnopencv.com/histogram-of-oriented-gradients/">histogram of oriented gradients (HOG)</a>
</li>
<li>hybrid approaches
</li>
</ul>

<p>
I experimented with each of these approaches, and with various
combinations of them, and finally selected a simple combination:
color space transformation and HOG-features.  Specifically, after
trying several different color spaces, I settled on the <a href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV</a> color
space and then performed HOG feature extraction on just the <i>V</i>
("value") channel.  Such a simple feature extractor may seem overly
simple&#x2014;and perhaps it is&#x2014;but the "proof is in the puding," as
they say.  It performed well, with decent accuracy on a test sample
(~98%) and on the project video.  Moreover, it has the virtue of
requiring relatively few computational resources.  Anything that
increases performance is a big win, since it promotes rapid,
<a href="https://en.wikipedia.org/wiki/Iterative_and_incremental_development">iterative</a> experimentation.  
</p>

<p>
Let's dive into some code to see how that went.
</p>

<p>
To set the stage, we were provided with two data archive files,
<a href="vehicles.zip">vehicles.zip</a> and <a href="non-vehicles.zip">non-vehicles.zip</a>, which as the names suggest
contained images of vehicles and things that are not vehicles.
</p>

<p>
Here is an example of a vehicle image.
</p>


<div class="figure">
<p><img src="vehicles/GTI_MiddleClose/image0000.png" alt="image0000.png" />
</p>
</div>

<p>
And, here is an example of a non-vehicle image.
</p>


<div class="figure">
<p><img src="non-vehicles/GTI/image1.png" alt="image1.png" />
</p>
</div>

<p>
The size of each image is 64 x 64 pixels, and the vehicle and
non-vehicle images are contained (after unpacking the archive files)
in directories <code>vehicle</code> and <code>non-vehicle</code>, respectively.  Now,
whatever classifier we use, we have to start by reading in these
images one way or another.  Confronted with tasks like this, I like
to compose small functions based on Python <a href="http://davidaventimiglia.com/python_generators.html">generators</a>, so first I
define a handful of useful utility functions.  
</p>

<dl class="org-dl">
<dt> feed </dt><dd>generator function over a <a href="https://docs.python.org/2/library/glob.html">glob</a>, which maps a value <code>y</code> to
each filename that matches <code>pattern</code>, yielding <a href="https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences">tuples</a>
</dd>
<dt> shuffle </dt><dd>list-builder function over a sequence of tuples, which
<a href="https://www.merriam-webster.com/dictionary/reify">reifies</a> it into a randomized list
</dd>
<dt> scale </dt><dd>non-generator function, which scales the values in an
array, either by the maximum value in the array or by a
supplied parameter <code>maxval</code>
</dd>
<dt> load </dt><dd>generator function over a sequnce of <a href="https://en.wikipedia.org/wiki/Ordered_pair">ordered pairs</a> in
which the first element is an image filename and the
second is any value (perhaps provided by the <code>feed</code>
function above), which loads the image files into NumPy
arrays
</dd>
<dt> flip </dt><dd>generator function over a sequence of ordered pairs in
which the first element is a NumPy array and the second is
any value, which "flips" the array horizontally (i.e.,
across a vertical axis) and producing a mirror image
</dd>
<dt> mirror </dt><dd>generator function over a sequence of ordered pairs as
in <code>flip</code>, but which yields first the entire sequence
unchanged and then the entire sequnce again but with the
images flipped
</dd>
</dl>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">feed</span> = <span style="color: #b4fa70;">lambda</span> pattern, y: ((f, y) <span style="color: #b4fa70;">for</span> f <span style="color: #b4fa70;">in</span> glob(pattern))
<span style="color: #fcaf3e;">shuffle</span> = <span style="color: #b4fa70;">lambda</span> l: sample(l, <span style="color: #e090d7;">len</span>(l))
<span style="color: #fcaf3e;">scale</span> = <span style="color: #b4fa70;">lambda</span> <span style="color: #fcaf3e;">img</span>,<span style="color: #fcaf3e;">maxval</span>=<span style="color: #e9b2e3;">None</span>: (img/np.<span style="color: #e090d7;">max</span>(img)*255).astype(np.uint8) <span style="color: #b4fa70;">if</span> maxval==<span style="color: #e9b2e3;">None</span> <span style="color: #b4fa70;">else</span> (img/maxval*255).astype(np.uint8)
<span style="color: #fcaf3e;">load</span> = <span style="color: #b4fa70;">lambda</span> g: ((mpimg.imread(x[0]),x[1]) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> g)
<span style="color: #fcaf3e;">flip</span> = <span style="color: #b4fa70;">lambda</span> g: ((x[0][:,::-1,:],x[1]) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> g)
<span style="color: #fcaf3e;">mirror</span> = <span style="color: #b4fa70;">lambda</span> g: chain(g, flip(g))
</pre>
</div>

<p>
When composed together, these functions provide a generator that
<a href="https://en.wikipedia.org/wiki/Lazy_loading">lazily loads</a> training images in random order, twice: first
unflipped, and second flipped.  This serves several related
purposes.  First, randomizing data for training purposes is a
best-practice in Machine Learning.  Second, it effectively doubles
the size of our training set.  Third, we anticipate encountering
vehicles on the road from any angle, where the vehicles themselves
are inherently symmetric across a vertical plane running
longitudinally down the length of the car.
</p>

<p>
Before we can use this generator, however, we need something to use
it on.  Let's define our functions for extracting features and for
creating our classifiers.
</p>

<p>
First, the <code>extract_features</code> function transforms a given image to a
target color space, performs HOG feature extraction on a target
color channel, then <a href="http://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range">scales</a> the features.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">extract_features</span>(img,
                     colorspace=cv2.COLOR_RGB2HSV,
                     channel=2,
                     orient=9,
                     pix_per_cell=8,
                     cell_per_block=4,
                     transform_sqrt=<span style="color: #e9b2e3;">False</span>,
                     feature_vec=<span style="color: #e9b2e3;">True</span>):
    <span style="color: #fcaf3e;">img</span> = scale(img)
    <span style="color: #fcaf3e;">X</span> = np.array([])
    <span style="color: #fcaf3e;">X</span> = np.append(X,
                  hog(cv2.cvtColor(img, colorspace)[:,:,channel],
                      orient,
                      (pix_per_cell,pix_per_cell),
                      (cell_per_block,cell_per_block),
                      transform_sqrt = transform_sqrt,
                      feature_vector = feature_vec))
    <span style="color: #fcaf3e;">s</span> = StandardScaler().fit(X)
    <span style="color: #b4fa70;">return</span> s.transform(X)
</pre>
</div>

<p>
Note that many of the parameters are supplied with default values.
That is no accident.  The values given above, and repeated here, are
the ones used throughout this project, and were obtained through
experimentation by trial-and-error.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Parameter</th>
<th scope="col" class="left">Value</th>
<th scope="col" class="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left"><code>colorspace</code></td>
<td class="left"><code>COLOR_RGB2HSV</code></td>
<td class="left">target color space</td>
</tr>

<tr>
<td class="left"><code>channel</code></td>
<td class="left"><code>2</code></td>
<td class="left">target color channel</td>
</tr>

<tr>
<td class="left"><code>orient</code></td>
<td class="left"><code>9</code></td>
<td class="left">HOG orientation bins</td>
</tr>

<tr>
<td class="left"><code>pix_per_cell</code></td>
<td class="left"><code>8</code></td>
<td class="left">pixels per HOG cell</td>
</tr>

<tr>
<td class="left"><code>cell_per_block</code></td>
<td class="left"><code>4</code></td>
<td class="left">cells per HOG block</td>
</tr>

<tr>
<td class="left"><code>transform_sqrt</code></td>
<td class="left"><code>False</code></td>
<td class="left">scale values by <code>math.sqrt</code></td>
</tr>

<tr>
<td class="left"><code>feature_vec</code></td>
<td class="left"><code>True</code></td>
<td class="left">return feature vector</td>
</tr>
</tbody>
</table>

<p>
Next, the <code>get_classifier</code> function returns a function which is
itself a trained classifier.  Parameters control whether or not to
train the classifier anew or to load a pre-trained classifier from a
file, and what the training/test set split should be when training a
new one.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_classifier</span>(<span style="color: #e090d7;">reload</span>=<span style="color: #e9b2e3;">False</span>,test_size=0.2):
    <span style="color: #b4fa70;">if</span> <span style="color: #e090d7;">reload</span>:
        <span style="color: #fcaf3e;">samples</span> = <span style="color: #e090d7;">list</span>(chain(feed(<span style="color: #e9b96e;">"vehicles/**/*.png"</span>,1),feed(<span style="color: #e9b96e;">"non-vehicles/**/*.png"</span>,0)))
        <span style="color: #fcaf3e;">data</span> = cycle(mirror(load(shuffle(samples))))
        <span style="color: #fcaf3e;">X_train</span>,<span style="color: #fcaf3e;">X_test</span>,<span style="color: #fcaf3e;">y_train</span>,<span style="color: #fcaf3e;">y_test</span> = train_test_split(*<span style="color: #e090d7;">zip</span>(*((extract_features(s[0]), s[1]) <span style="color: #b4fa70;">for</span> s <span style="color: #b4fa70;">in</span> islice(data, <span style="color: #e090d7;">len</span>(samples)))), test_size=test_size, random_state=np.random.randint(0, 100))
        <span style="color: #fcaf3e;">svc</span> = LinearSVC()
        svc.fit(X_train, y_train)
        pickle.dump(svc, <span style="color: #e090d7;">open</span>(<span style="color: #e9b96e;">"save.p"</span>,<span style="color: #e9b96e;">"wb"</span>))
        <span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">'Test Accuracy of SVC = '</span>, <span style="color: #e090d7;">round</span>(classifier.score(X_test, y_test), 4))
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #fcaf3e;">svc</span> = pickle.load(<span style="color: #e090d7;">open</span>(<span style="color: #e9b96e;">"save.p"</span>, <span style="color: #e9b96e;">"rb"</span>))
    <span style="color: #b4fa70;">return</span> svc
</pre>
</div>

<p>
Note the use of our composable utility functions to load the data
<a href="#coderef-compose1"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-compose1');" onmouseout="CodeHighlightOff(this, 'coderef-compose1');">here</a> and <a href="#coderef-compose2"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-compose2');" onmouseout="CodeHighlightOff(this, 'coderef-compose2');">here</a>.  Note also that there are a variety of classifiers we
could use.
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine (SVM)</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
</li>
</ul>

<p>
I was prepared to experiment with each of these, and perhaps with
their combinations.  I started with an SVG, however, and found that
it performed well all on its own.
</p>

<p>
Training a classifier now is as simple as
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">classifier</span> = get_classifier(<span style="color: #e9b2e3;">True</span>)
</pre>
</div>

<pre class="example">
Test Accuracy of SVC =  0.9938
</pre>

<p>
while loading a saved classifier is even simpler
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">classifier</span> = get_classifier()
</pre>
</div>

<p>
Saving and re-loading classifiers like in this second command was
very helpful in this project for promoting rapid iteration, because
once I had a classifier I was happy with, I could move onto
subsequent stages in the project without needing to retrain the
classifier every time. 
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Sliding Window Search I</h2>
<div class="outline-text-2" id="text-4">
<p>
I performed the most experimentation on various sliding window
schemes.  Initially, I expended effort on behalf of a single idea:
<i>Can I model vehicle position not in screen coordinates, measured in
pixels, but rather in real-world coordinates, measured in meters?</i>
My strategy was to generate sliding windows on a three-dimensional
(3D) grid whose origin is where the camera is placed and whose units
are meters, and then use geometry to project those windows onto the
screen in pixel coordinates.  This model has these assumptions.
</p>

<ul class="org-ul">
<li>The image plane roughly corresponds to the vehicle's windshield.
</li>
<li>The windshield is approximately 2 meters wide, 1 meter tall, and 2
meters above the road.
</li>
<li>The camera is placed approximately 1 meter behind the windshield's
center, with a line-of-sight (LOS) perpendicular to it.
</li>
<li>The grid coordinages \(\left( x, y, z \right)_{grid}\) correspond to
the horizontal position across the road, the vertical position
above the road, and the logitudinal position down the road.
</li>
<li>Positive \(x\) values are to the right, negative \(x\) values are to
the left, and \(x_{grid} \in \left[ -15, 15 \right]\).
</li>
<li>Negative \(y\) values are below the camera, and \(y_{grid} \in \left[
    -2, 0 \right]\).
</li>
<li>\(z \lt 1\) values are inside the car, and \(z_{grid} \in \left[ 10,
    100 \right]\).
</li>
</ul>

<p>
These assumptions determine the geometry of the problem and set its
physical scale, with a field-of-view (FOV) of 90°, and allow us to
create sliding windows as described above.  In principle, vehicle
detections on image patches can then be assigned real-world
coordinates \((x, y, z)\), or at least road coordinates \((x,
  z)_{y=0}\), a real-world "heat map" can be built up, and then
individual vehicles can be identified either with conventional
thresholds + <a href="http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html">labelling</a>, with a <a href="https://en.wikipedia.org/wiki/Blob_detection">Laplace-of-Gaussian</a> technique, or
with <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.vq.html"><i>k</i>-means clustering</a>.
</p>

<p>
The following coordinate conversion functions support the geometrical model outlined
above.
</p>

<dl class="org-dl">
<dt> crt2cyl </dt><dd>Cartesian-to-cylindrical
</dd>
<dt> cyl2crt </dt><dd>cylindrical-to-Cartesian
</dd>
<dt> cyl2sph </dt><dd>cylindrical-to-spherical
</dd>
<dt> sph2cyl </dt><dd>spherical-to-cylindrical
</dd>
<dt> crt2sph </dt><dd>Cartesian-to-spherical
</dd>
<dt> sph2crt </dt><dd>spherical-to-Cartesian
</dd>
</dl>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">crt2cyl</span> = <span style="color: #b4fa70;">lambda</span> x,y,z: (math.sqrt(x**2+y**2), math.atan2(y,x), z)
<span style="color: #fcaf3e;">cyl2crt</span> = <span style="color: #b4fa70;">lambda</span> rho,phi,z: (rho*math.cos(phi), rho*math.sin(phi), z)
<span style="color: #fcaf3e;">cyl2sph</span> = <span style="color: #b4fa70;">lambda</span> rho,phi,z: (math.sqrt(rho**2+z**2), math.atan2(rho, z), phi)
<span style="color: #fcaf3e;">sph2cyl</span> = <span style="color: #b4fa70;">lambda</span> r,theta,phi: (r*math.sin(theta), phi, r*math.cos(theta))
<span style="color: #fcaf3e;">crt2sph</span> = <span style="color: #b4fa70;">lambda</span> x,y,z: (math.sqrt(x**2+y**2+z**2), math.acos(z/math.sqrt(x**2+y**2+z**2)), math.atan2(y,x))
<span style="color: #fcaf3e;">sph2crt</span> = <span style="color: #b4fa70;">lambda</span> r,theta,phi: (r*math.sin(theta)*math.cos(phi), r*math.sin(theta)*math.sin(phi), r*math.cos(theta))
</pre>
</div>

<p>
The <code>get_window</code> function computes from \((x,y,z)\) a "window", which
is a list of tuples wherein the first two provide the corners of its
"bounding box" and the last provides the coordinates of its center.
Note that it also takes <code>height</code> and <code>width</code> parameters for the
physical size (in meters) of the window, as well as a <code>horizon</code>
parameter, which is the fraction of the image plane (from below) at
which the horizon appears.  The default value of <code>0.5</code> corresponds
to the middle.  Finally, like many of my functions it takes a NumPy
image array parameter, <code>img</code>, which is mainly for extracting the
size and shape of the image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_window</span>(img, x, y, z, horizon=0.5, width=2, height=2):
    <span style="color: #fcaf3e;">d</span> = 1
    <span style="color: #fcaf3e;">r</span>,<span style="color: #fcaf3e;">theta</span>,<span style="color: #fcaf3e;">phi</span> = crt2sph(x,y,z)
    <span style="color: #fcaf3e;">rho2</span> = d*math.tan(theta)
    <span style="color: #fcaf3e;">x2</span>,<span style="color: #fcaf3e;">y2</span> = (rho2*math.cos(phi),rho2*math.sin(phi))
    <span style="color: #fcaf3e;">center</span> = (<span style="color: #e090d7;">int</span>(img.shape[1]*0.5+x2*img.shape[1]//2),
              <span style="color: #e090d7;">int</span>(img.shape[0]*(1-horizon)-y2*img.shape[1]//2))
    <span style="color: #fcaf3e;">scale</span> = img.shape[1]//2
    <span style="color: #fcaf3e;">dx</span> = <span style="color: #e090d7;">int</span>(width/2*scale/z)
    <span style="color: #fcaf3e;">dy</span> = <span style="color: #e090d7;">int</span>(height/2*scale/z)
    <span style="color: #fcaf3e;">window</span> = [(center[0]-dx,center[1]-dy), (center[0]+dx,center[1]+dy)] + [(x,y,z)]
    <span style="color: #b4fa70;">return</span> window
</pre>
</div>

<p>
Next, the <code>draw_window</code> function annotates an image <code>img</code> with the
window <code>bbox</code>.  This does not factor into the actual vehicle
detection, of course, but the visualization is valuable for
understanding how the video processsing pipeline ultimately is
working.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">draw_window</span>(img, bbox, color=(0,0,255), thick=3):
    cv2.rectangle(img, bbox[0], bbox[1], color, thick)
    <span style="color: #b4fa70;">return</span> img
</pre>
</div>

<p>
For example, first we draw a window box that roughly corresponds to
the windshield itself, using a test image from the lecture notes.
The windshield's center is at real-world coordinages
\((x,y,z)_{windshield} = (0,0,1)\).
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
draw_window(image, get_window(image, 0, 0.0, 1, horizon=0.5, width=2, height=1))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/windshield.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/windshield.png" alt="windshield.png" width="800px" />
</p>
</div>

<p>
Next, we draw window boxes around a few of the cars in the image.
Note that here we are eschewing the default value of <code>horizon</code> in
favor of <code>0.28</code>, given the peculiar tilt the camera seems to have in
this image.  That value, like the real-world vehicle coordintes
\((x,y,z)_i\), were obtained by hand through trial-and-error.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
draw_window(image, get_window(image, 4.1, -1.0, 8, horizon=0.28))
draw_window(image, get_window(image, -10.5, -1.0, 22, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 32, horizon=0.28))
draw_window(image, get_window(image, -0.8, -1.0, 35, horizon=0.28))
draw_window(image, get_window(image, 3, -1.0, 55, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 55, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 70, horizon=0.28))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/bbox-example-image-test.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/bbox-example-image-test.png" alt="bbox-example-image-test.png" width="800px" />
</p>
</div>

<p>
In order to help visualize the geometry further, we animate a
handful of windows receding into the distance.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">zooming_windows</span>(img):
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">make_frame</span>(t):
        <span style="color: #fcaf3e;">frame</span> = np.copy(img)
        <span style="color: #fcaf3e;">z</span> = 2**(t % 5)*5
        draw_window(frame, get_window(frame,-10.5,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,-6.1,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,-0.8,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,4.1,-1.0,z,horizon=0.28))
        cv2.putText(frame, <span style="color: #e9b96e;">"z: %.2f m"</span> % z, (50,50), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2)
        <span style="color: #b4fa70;">return</span> frame
    <span style="color: #b4fa70;">return</span> make_frame

<span style="color: #fcaf3e;">clip</span> = VideoClip(zooming_windows(mpimg.imread(<span style="color: #e9b96e;">'bbox-example-image.jpg'</span>)), duration=5)
clip.write_videofile(<span style="color: #e9b96e;">"output_images/zooming-windows.mp4"</span>, fps=25)
</pre>
</div>

<iframe width="560" height="315" src="https://www.youtube.com/embed/lqp9rOSPVrc" frameborder="0" allowfullscreen></iframe>

<p>
This is just for visualization.  For vehicle detection, a denser
grid should be used, and we raster the windows horizontally as they
ratchet down-range.  We also confine the windows to a horizontal
plane, at \(z = -1\).  But, because this sliding window and other ones
like it actually will be used in the vehicle-detection
video-processing pipeline, it is worthwhile to remove windows that
exceed the image boundary.  That is the purpose of the <code>clip_window</code>
function.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">clip_window</span> = <span style="color: #b4fa70;">lambda</span> x, img: <span style="color: #e090d7;">sum</span>([0&lt;=x[0][0]&lt;=image.shape[1],
                                  0&lt;=x[1][0]&lt;=image.shape[1],
                                  0&lt;=x[0][1]&lt;=image.shape[0],
                                  0&lt;=x[1][1]&lt;=image.shape[0]])==4
</pre>
</div>


<p>
Since our strategy will be to write functions to produce "grids"
that can be used both for visualization and for vehicle-detection,
we refactor much of the animated visualization into a new function,
<code>get_frame_maker</code>.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_frame_maker</span>(img, grid):
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">make_frame</span>(t):
        <span style="color: #fcaf3e;">frame</span> = np.copy(img)
        draw_window(frame, grid.__next__()[:2], color=(0,255,255))
        <span style="color: #b4fa70;">return</span> frame
    <span style="color: #b4fa70;">return</span> make_frame
</pre>
</div>

<p>
With these tools, first we define a "sparse grid"
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">sparse_scan</span>(img):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:2,-1.0:0:2,3:7:1]
    <span style="color: #fcaf3e;">grid</span>[2,]=2**grid[2,]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, img), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<p>
visualize its 40 windows
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), sparse_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/sparse-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
40
</pre>


<div class="figure">
<p><img src="output_images/sparse-scan.png" alt="sparse-scan.png" width="800px" />
</p>
</div>

<p>
and then animate them.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">clip</span> = VideoClip(get_frame_maker(image, cycle(sparse_scan(image))), duration=10)
clip.write_videofile(<span style="color: #e9b96e;">"output_images/sparse-scan.mp4"</span>, fps=25)
</pre>
</div>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Vn1HxPRd2W0" frameborder="0" allowfullscreen></iframe>

<p>
We can also define a "dense grid" with more windows, scanning the
roadway with finer resolution in the \(x\) and \(z\) directions.  We
skip the animation this time, as it is rather boring.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">dense_scan</span>(img, h=2,w=2):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:0.5,-1.0:0:2,10:100:2]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28, height=h, width=w)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, img), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<p>
When produce the grid image, note that it has 2600+ windows!  That
probably is excessive and would slow down video processing.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), dense_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/dense-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
2653
</pre>


<div class="figure">
<p><img src="output_images/dense-scan.png" alt="dense-scan.png" width="800px" />
</p>
</div>

<p>
The sparse scan above probably is too sparse, but one way we can
reduce the number of windows would be to search the perimeter of the
road, where new cars are likely to come on-stage.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">perimeter_scan</span>(img):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:0.5,-1.0:0:2,10:100:2]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: <span style="color: #b4fa70;">not</span> (-4&lt;=x[0]&lt;=4 <span style="color: #b4fa70;">and</span> 5&lt;=x[2]&lt;=40), grid))
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, img), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), perimeter_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/perimeter-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
2381
</pre>

<p>
Sadly, this barely makes a dent in reducing the number of windows.  
</p>


<div class="figure">
<p><img src="output_images/perimeter-scan.png" alt="perimeter-scan.png" width="800px" />
</p>
</div>

<p>
In order to make headway, a simple choice is just to stick with the
dense grid, perform vehicle detections with it against a test image,
and gauge its performance.
</p>

<p>
To do that, we just map our classifier over all of the window
patches on an image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">process</span>(x):
    <span style="color: #b4fa70;">return</span> (classifier.predict(extract_features(x[0]))[0],x[1])

<span style="color: #fcaf3e;">results</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(process, get_patches(image, dense_scan(image))))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(results))
</pre>
</div>

<pre class="example">
... &gt;&gt;&gt; &gt;&gt;&gt; 2653
</pre>

<p>
To visulize where vehicle detections have occurred on our dense grid
over the road, we filter the processed results that have a value
greater than 1 (i.e., a detection has occurred for that window patch)
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">_</span>,<span style="color: #fcaf3e;">r</span> = <span style="color: #e090d7;">zip</span>(*<span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: x[0]&gt;0, results))
<span style="color: #fcaf3e;">x</span>,<span style="color: #fcaf3e;">y</span>,<span style="color: #fcaf3e;">z</span> = <span style="color: #e090d7;">zip</span>(*r)
plt.scatter(x,z,s=10,c=y)
</pre>
</div>


<div class="figure">
<p><img src="output_images/figure_4.png" alt="figure_4.png" width="800px" />
</p>
</div>

<p>
These results are interesting and suggestive.  The contiguous
regions of detections presumably correspond to vehicles, with their
2D location associated with the "center" of each island.  However,
projection effects seem to elongate the detected regions with a
strong, pronounced radial pattern, which could be problematic.
Perhaps with suitable thresholding, a technique as simple as the
<code>label</code> function would be sufficient for picking out the cars.  On
the other hand, we might need more sophisticated techniques, such as
<a href="https://en.wikipedia.org/wiki/Blob_detection">Laplace-of-Gaussian</a> or with <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.vq.html"><i>k</i>-means clustering</a>.  This is an
intriguing direction of inquiry to pursue in further studies,
however in this one I found that I was running out of time.  
</p>

<p>
So, I switched gears to a more traditional sliding windows approach.
</p>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Sliding Window Search II</h2>
<div class="outline-text-2" id="text-5">
<p>
To refresh the reader, a more traditional sliding windows approach
models a grid of windows and their image patches not in real-world
3D physical space, but in 2D image space.  This involves trade-offs.
On the one hand, we give up a straightforward 3D interpretation of a
vehicle detection event.  In principle, we could still recover
distance information by deprojecting the window (the reverse of our
operation above), but at the expense of greater complication.  On
the other hand, we gain with this trade-off a simpler implementation
that already has a proven track-record.
</p>

<p>
We can reuse much of our other code, though, since we just need to
define functions to produce grids that obey whatever selection
functions we desire.
</p>

<p>
First up is a simple "image-plane scan", which carpets the image
plane in a uniform grid of windows at varous fixed scales.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">image_plane_scan</span>(img,ny,overlap,scale):
    <span style="color: #fcaf3e;">size</span> = <span style="color: #e090d7;">int</span>(img.shape[0]//ny)//scale
    <span style="color: #fcaf3e;">delta</span> = <span style="color: #e090d7;">int</span>(size*(1-overlap))
    <span style="color: #fcaf3e;">box1</span> = (0,
            img.shape[1],
            (img.shape[0]-img.shape[0]//scale)//2,
            img.shape[0] - (img.shape[0]-img.shape[0]//scale)//2)
    <span style="color: #fcaf3e;">box2</span> = (0,
            img.shape[1],
            (img.shape[0]//2),
            img.shape[0])
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[0:img.shape[1]:delta,
                    img.shape[0]:-delta:-delta].T.reshape(-1,2)
    <span style="color: #fcaf3e;">grid</span> = ([(c[0],c[1]), (c[0]+size,c[1]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box1), grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box2), grid)
    <span style="color: #b4fa70;">return</span> grid

<span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>()
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       image_plane_scan(image,4,0.50,1),
                       image_plane_scan(image,4,0.50,2),
                       image_plane_scan(image,4,0.50,3)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/imageplane-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... &gt;&gt;&gt; &gt;&gt;&gt;
... ... ... ... ... ... Number of windows: 1439
</pre>


<div class="figure">
<p><img src="output_images/imageplane-scan.png" alt="imageplane-scan.png" width="800px" />
</p>
</div>

<p>
This produces 1400+ images, which highlights a persistent problem I
grappled with.  There is an inherent trade-off between the accuracy
of a dense window sample, and the performance of a sparse window sample.
</p>

<p>
A conjecture I had to help ease the tension between these two poles
was to relax the constraint of a regular grid of windows in favor of
a random scattering of windows.  One of the reasons the window count
soared with a regular grid was the overlap; a high degree of overlap
(&gt;50%) was needed for higher spatial resolution of detected vehicle
locations, but the number of windows is essentially quadratic in the
degree of overlap.  However, the stochastic behavior of an irregular
random sampling of windows means that a higher spatial resolution
can be achieved in an economy of window patches.  
</p>

<p>
The trade-offs here, however, are two-fold.  First, we no longer can
pre-compute the grid, but instead must compute a new random ensemble
of windows for each video frame.  In the testing that I did, this
proved to be of little concern; the Python profiler, and experience
as well, showed that the grid computation time was relatively
trivial.  The bulk of the time was spent on feature extraction and
classification for each window patch, a task that obviously cannot
be precomputed irrespective of the grid strategy.
</p>

<p>
Second, since any one <i>particular</i> frame is treated with a
relatively sparse (but now random) irregular grid of windows, this
intensifies the need for integrating the signal over multiple frames
(a task we anticipated in any case).  Consequently, we lose
resolution in the time domain.  While that could be a problem for
fast-moving vehicles, it was not for the relatively slow relative
velocity of the vehicles in our project video.
</p>

<p>
My first version of a random scan uses a region-of-interest mask
that selects out a trapezoidal region covering just the border of
the road.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">region_of_interest</span>(img, vertices):
    <span style="color: #fcaf3e;">mask</span> = np.zeros_like(img)   
    <span style="color: #b4fa70;">if</span> <span style="color: #e090d7;">len</span>(img.shape) &gt; 2:
        <span style="color: #fcaf3e;">channel_count</span> = img.shape[2]
        <span style="color: #fcaf3e;">ignore_mask_color</span> = (255,) * channel_count
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #fcaf3e;">ignore_mask_color</span> = 255
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    <span style="color: #fcaf3e;">masked_image</span> = cv2.bitwise_and(img, mask)
    <span style="color: #b4fa70;">return</span> masked_image
</pre>
</div>

<p>
The actual function <code>random_scan</code> takes an image <code>img</code> (again, just
for the size information), and a window size.  Since these we are
now operating in the pixel coordinates of the image plane rather
than in the physical coordinates of the real world, the window size
is taken just in pixels.  This function works by thresholding a
random array.  It is a somewhat elegant technique, but is
inefficient and <i>slowww</i>.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan</span>(img,size):
    <span style="color: #fcaf3e;">x</span> = np.random.rand(*img.shape[:2])
    <span style="color: #fcaf3e;">x</span>[x&lt;0.999] = 0
    <span style="color: #fcaf3e;">x</span> = scale(np.ceil(x))
    <span style="color: #fcaf3e;">x</span> = region_of_interest(x, np.array([[[0, 0.5*image.shape[0]],
                           [image.shape[1], 0.5*image.shape[0]],
                           [image.shape[1], image.shape[0]],
                           [(1-2/6)*image.shape[1], 0.5*image.shape[0]],
                           [(2/6)*image.shape[1], 0.5*image.shape[0]],
                           [0, image.shape[0]]]]).astype(<span style="color: #e9b96e;">'int'</span>))
    <span style="color: #fcaf3e;">x</span> = np.dstack(np.nonzero(x))
    <span style="color: #fcaf3e;">s</span> = np.random.choice(2**np.arange(4), <span style="color: #e090d7;">len</span>(x[0]))
    <span style="color: #fcaf3e;">grid</span> = ([(c[1],c[0]),
             (c[1]+size,c[0]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> x[0])
    <span style="color: #b4fa70;">return</span> grid

<span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>()
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan(image,180),
                       random_scan(image,90),
                       random_scan(image,60)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan1.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... ... ... ... ... ... ... ... ... &gt;&gt;&gt; &gt;&gt;&gt;
... ... ... ... ... ... Number of windows: 459
</pre>


<div class="figure">
<p><img src="output_images/random-scan1.png" alt="random-scan1.png" width="800px" />
</p>
</div>

<p>
The next random grid function <code>random_scan2</code>, uses a slightly
less-elegant approach, but is noticeably faster.  Aside from
confining the window to the bottom half of the image, however, it
does not use a region-of-interest mask.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan2</span>(img,size,num=100):
    <span style="color: #fcaf3e;">x</span> = np.random.rand(num,2)
    <span style="color: #fcaf3e;">x</span>[:,0]*=image.shape[1]
    <span style="color: #fcaf3e;">x</span>[:,1]*=image.shape[1]
    <span style="color: #fcaf3e;">x</span> = x.astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #fcaf3e;">x</span> = x[x[:,1]&lt;image.shape[0]]
    <span style="color: #fcaf3e;">x</span> = x[x[:,1]&gt;=image.shape[0]//2]
    <span style="color: #fcaf3e;">box</span> = (0,img.shape[1],(img.shape[0]//2),650)
    <span style="color: #fcaf3e;">grid</span> = ([(c[0],c[1]),
             (c[0]+size,c[1]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> x)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan2(image,256,1000),
                       random_scan2(image,128,1000),
                       random_scan2(image,64,1000)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan2.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... Number of windows: 303
</pre>


<div class="figure">
<p><img src="output_images/random-scan2.png" alt="random-scan2.png" width="800px" />
</p>
</div>

<p>
The next random scanner I tried worked in polar (pixel) coordinates
so as to achieve a masking affect, that concentrates windows on the
road borders where vehicles are most likely to appear.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan3</span>(img,size,num=100,minr=<span style="color: #e9b2e3;">None</span>,maxr=<span style="color: #e9b2e3;">None</span>,mintheta=<span style="color: #e9b2e3;">None</span>,maxtheta=<span style="color: #e9b2e3;">None</span>,center=<span style="color: #e9b2e3;">None</span>,scale=<span style="color: #e9b2e3;">True</span>):
    <span style="color: #b4fa70;">if</span> center==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">center</span> = <span style="color: #e090d7;">tuple</span>(np.array(image.shape[:2][::-1])//2)
    <span style="color: #fcaf3e;">polar</span> = np.random.rand(num,2)
    <span style="color: #fcaf3e;">polar</span>[:,0]*=image.shape[1]
    <span style="color: #fcaf3e;">polar</span>[:,1]*=math.pi*2
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> minr==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,0]&gt;=minr]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> maxr==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,0]&lt;maxr]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> mintheta==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,1]&gt;=0]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> maxtheta==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,1]&lt;maxtheta]
    <span style="color: #b4fa70;">if</span> scale:
        <span style="color: #fcaf3e;">s</span> = (size//2*polar[:,0]/image.shape[1]).astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #b4fa70;">try</span>:
            <span style="color: #fcaf3e;">dist</span> = <span style="color: #e090d7;">int</span>(math.sqrt(<span style="color: #e090d7;">sum</span>([(center[0]-image.shape[1]//2)**2,
                                      (center[1]-image.shape[0]//2)**2])))
            <span style="color: #fcaf3e;">s</span> = [<span style="color: #e090d7;">int</span>(size*(dist/(image.shape[1]//2)))]*<span style="color: #e090d7;">len</span>(polar)
        <span style="color: #b4fa70;">except</span>:
            pdb.set_trace()
    <span style="color: #fcaf3e;">x</span>,<span style="color: #fcaf3e;">y</span>=<span style="color: #e090d7;">zip</span>(*np.dstack((center[0]+polar[:,0]*np.cos(polar[:,1]),
                        center[1]+polar[:,0]*np.sin(polar[:,1]))).astype(<span style="color: #e9b96e;">'int'</span>)[0])
    <span style="color: #fcaf3e;">grid</span> = ([(c[0]-c[2],c[1]-c[2]), (c[0]+c[2],c[1]+c[2])] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> <span style="color: #e090d7;">zip</span>(x,y,s))
    <span style="color: #fcaf3e;">box</span> = (0,img.shape[1],(0),670)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan3(image,image.shape[1]//4,
                                    3000,
                                    minr=image.shape[0]//3,
                                    mintheta=0,
                                    maxtheta=math.pi)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan3.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... ... ... Number of windows: 200
</pre>


<div class="figure">
<p><img src="output_images/random-scan3.png" alt="random-scan3.png" width="800px" />
</p>
</div>

<p>
This produces an interestng pattern, but I was not comfortable
peculiar way the windows are scaled to different sizes, so I wrote
yet another grid window function <code>random_scan4</code>, which is a bit of a
hybrid.  It actually re-uses the 3D model described above in
<b>Sliding Window Search I</b>.  Windows are defined in a 3D volume which
covers the road from left to right, from the car to the horizon, and
from the camera level down to the road.  I.e., it is like a long,
thick "ribbon", within which windows are randomly sampled.  As
above, we are back in physical space for window sizes, rather than
in pixel space.  Finally, the physical space windows are projected
back onto the image plane to give us a grid window in pixel-space.
In fact, this is almost exactly as we did in the earlier section.
The main differences are:
</p>

<ol class="org-ol">
<li>We discard the 3D window location information after projecting it
to a grid window on the image plane.
</li>
<li>Window locations are randomly drawn from the 3D volume described
above, rather than laid out in a regular array.
</li>
</ol>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan4</span>(img,size,num=100,width=25,left=-12.5):
    <span style="color: #fcaf3e;">grid</span> = np.random.rand(num,3)
    <span style="color: #fcaf3e;">grid</span>[:,0]*=width
    <span style="color: #fcaf3e;">grid</span>[:,1]*=2
    <span style="color: #fcaf3e;">grid</span>[:,1]-=2
    <span style="color: #fcaf3e;">grid</span>[:,2]*=40
    <span style="color: #fcaf3e;">grid</span>[:,0]+=left
    <span style="color: #fcaf3e;">grid</span>[:,1]-=4
    <span style="color: #fcaf3e;">grid</span>[:,2]+=5
    <span style="color: #fcaf3e;">grid</span> = grid.astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], height=4, width=4)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window2(x, img), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), random_scan4(image,2,1000)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan4.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
825
</pre>


<div class="figure">
<p><img src="output_images/random-scan4.png" alt="random-scan4.png" width="800px" />
</p>
</div>

<p>
Note that the above function takes parameters <code>width</code> and <code>left</code>
which set the width of the "ribbon" volume, and its left edge (in
meters).  We can easily combine a couple calls to this grid
generating function with judicious parameter choices in order to
achive interesting search patterns.  For instance, in
<code>random_scan5</code>, we superimpose two ribbons, one on the left, and one
on the right, in order just to search the road borders.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan5</span>(img,size,num=100):
    <span style="color: #fcaf3e;">grid</span> = chain(random_scan4(img,size,num//2,width=20,left=-30),
                 random_scan4(img,size,num//2,width=20,left=+10))
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), random_scan5(image,2,1000)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan5.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
588
</pre>


<div class="figure">
<p><img src="output_images/random-scan5.png" alt="random-scan5.png" width="800px" />
</p>
</div>

<p>
While we may not want to search in this way in general, since the
void in the middle is a giant "blind spot", as a visualization this
has the nice property of removing foreground windows so that the way
they naturally scale with distance is revealed.  
</p>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Video Implementation</h2>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7">Discussion</h2>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8">Tips</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: David A. Ventimiglia (<a href="mailto:dventimi@gmail.com">dventimi@gmail.com</a>)</p>
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-03-08&gt;</span></span></p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
