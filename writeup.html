<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Vehicle Detection Project</title>
<!-- 2017-03-09 Thu 16:10 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="David A. Ventimiglia" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<style>@import 'https://fonts.googleapis.com/css?family=Quattrocento';</style>
<link rel="stylesheet" type="text/css" href="base.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Vehicle Detection Project</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
The goals / steps of this project are the following:
</p>

<ul class="org-ul">
<li>Perform a Histogram of Oriented Gradients (HOG) feature extraction
on a labeled training set of images and train a Linear SVM
classifier
</li>
<li>Normalize features and randomize a selection for training and
testing.
</li>
<li>Implement a sliding-window technique and use trained classifier to
search for vehicles in images.
</li>
<li>Run pipeline on a video stream and create a heat map of recurring
detections frame by frame to reject outliers and follow detected
vehicles.
</li>
<li>Estimate a bounding box for vehicles detected.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Setup</h2>
<div class="outline-text-2" id="text-2">
<p>
The initial setup includes creating the <a href="https://www.python.org/">Python</a> environment with the
packages that the project needs and uses.
</p>

<p>
<b>NOTE I</b>: The code for this project can be found in the repository's
<a href="detect.py">detect.py</a> file.  <i>However</i>, all of the code in that file was
generated directly from the code blocks that appear in this file and
as such, contain no new information.  Reviewers can consult either
that Python file or this document.
</p>

<p>
<b>NOTE II</b>: This document is presented in a variety of formats.
There is this Emacs Org-Mode <a href="writeup.html">file</a>, a <a href="writeup.pdf">PDF</a> generated using <i>LaTeX</i>, an
<a href="writeup.html">HTML</a> file, and a <a href="writeup.md">Markdown</a> file.  The Markdown file will be rendered
directly by GitHub when viewed on the web.  The HTML version can be
rendered either by cloning the repository to your own computer and
opening the file in a browser locally.  Or, you can view the same
file in GitHub Pages at <a href="https://dventimi.github.io/CarND-Advanced-Lane-Lines/writeup.html">this link</a>.  It looks quite a bit better than
the GitHub-rendered Markdown version.
</p>

<dl class="org-dl">
<dt> <a href="http://matplotlib.org/">matplotlib</a> </dt><dd>plotting and image processing tools
</dd>
<dt> <a href="http://www.numpy.org/">NumPy</a> </dt><dd>foundational scientific computing library
</dd>
<dt> <a href="http://zulko.github.io/moviepy/">MoviePy</a> </dt><dd>video processing tools
</dd>
<dt> <a href="http://opencv.org/">OpenCV</a> </dt><dd>computer vision library
</dd>
</dl>

<p>
The <a href="https://github.com/">GitHub</a> <a href="https://github.com/dventimi/CarND-Advanced-Lane-Lines">repository</a> for this project contains an <a href="environment.yml">environment.yml</a>
file that can be used to create and activate a <a href="https://conda.io/docs/">Conda</a> environment
with these commands.
</p>

<div class="org-src-container">

<pre class="src src-sh">conda create --name CarND-Vehicle-Detection --clone CarND-Advanced-Lane-Lines
<span style="color: #e090d7;">source</span> activate CarND-Vehicle-Detection
conda env export &gt; environment.yml
</pre>
</div>

<div class="org-src-container">

<pre class="src src-sh">conda env create --file environment.yml --name CarND-Vehicle-Detection
<span style="color: #e090d7;">source</span> activate CarND-Vehicle-Detection
</pre>
</div>

<p>
Once activated this environment is used to launch Python in whatever
way one likes, such as a <a href="https://www.python.org/shell/">Python shell</a>, a <a href="https://ipython.org/">IPython shell</a>, or a <a href="http://jupyter.org/">jupyter
notebook</a>.  Having done that, the usual first step is to import the
packages that are used.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">from</span> glob <span style="color: #b4fa70;">import</span> glob
<span style="color: #b4fa70;">from</span> itertools <span style="color: #b4fa70;">import</span> groupby, islice, zip_longest, cycle, filterfalse, chain
<span style="color: #b4fa70;">from</span> moviepy.editor <span style="color: #b4fa70;">import</span> VideoFileClip, VideoClip
<span style="color: #b4fa70;">from</span> random <span style="color: #b4fa70;">import</span> choice, sample
<span style="color: #b4fa70;">from</span> scipy.ndimage.measurements <span style="color: #b4fa70;">import</span> label
<span style="color: #b4fa70;">from</span> skimage.feature <span style="color: #b4fa70;">import</span> hog
<span style="color: #b4fa70;">from</span> sklearn.model_selection <span style="color: #b4fa70;">import</span> train_test_split
<span style="color: #b4fa70;">from</span> sklearn.preprocessing <span style="color: #b4fa70;">import</span> StandardScaler
<span style="color: #b4fa70;">from</span> sklearn.svm <span style="color: #b4fa70;">import</span> LinearSVC
<span style="color: #b4fa70;">import</span> cv2
<span style="color: #b4fa70;">import</span> math
<span style="color: #b4fa70;">import</span> matplotlib.image <span style="color: #b4fa70;">as</span> mpimg
<span style="color: #b4fa70;">import</span> matplotlib.pyplot <span style="color: #b4fa70;">as</span> plt
<span style="color: #b4fa70;">import</span> numpy <span style="color: #b4fa70;">as</span> np
<span style="color: #b4fa70;">import</span> pdb
<span style="color: #b4fa70;">import</span> pickle
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Histogram of Oriented Gradients (HOG)</h2>
<div class="outline-text-2" id="text-3">
<p>
The first step in this project was to create a vehicle classifier
that was capable of identifying vehicles in an image, since we can
treat the frames of the video we are processing as individual
images.  Broadly, there are two main approaches for this task.  
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Neural Networks</a>
</li>
</ul>

<p>
Because the emphasis of this module in the Udacity course seemed to
focus on the computer vision approach, and because we already used
neural networks for two previous projects, I chose to explore the
first of these.
</p>

<p>
Though the two are similar (and have the same objective:
classification), one of the hallmarks of the computer vision
approach seems to be manual <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>: relying on human
experience to select useful features for a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> problem.
In the class, we explored several.
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Color_space">color space</a> selection
</li>
<li><a href="https://en.wikipedia.org/wiki/Data_binning">binning</a> of spatial features
</li>
<li><a href="https://en.wikipedia.org/wiki/Color_histogram">color histograms</a>
</li>
<li><a href="http://www.learnopencv.com/histogram-of-oriented-gradients/">histogram of oriented gradients (HOG)</a>
</li>
<li>hybrid approaches
</li>
</ul>

<p>
I experimented with each of these approaches, and with various
combinations of them, and finally selected a simple combination:
color space transformation and HOG-features.  Specifically, after
trying several different color spaces, I settled on the <a href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV</a> color
space and then performed HOG feature extraction on just the <i>V</i>
("value") channel.  Such a simple feature extractor may seem overly
simple&#x2014;and perhaps it is&#x2014;but the "proof is in the pudding," as
they say.  It performed well, with decent accuracy on a test sample
(~98%) and on the project video.  Moreover, it has the virtue of
requiring relatively few computational resources.  Anything that
increases performance is a big win, since it promotes rapid,
<a href="https://en.wikipedia.org/wiki/Iterative_and_incremental_development">iterative</a> experimentation.  
</p>

<p>
Moreover, for the HOG parameters (orientations, pixels<sub>per</sub><sub>cell</sub>,
cells<sub>per</sub><sub>block</sub>), I started with the values that we used in the
quizzes in the lectures, and then manually tuned toward better
values by simple trial-and-error.  Because the classifier seemed to
do well on the test set, not much tuning was necessary.
</p>

<p>
Let's dive into some code to see how that went.
</p>

<p>
To set the stage, we were provided with two data archive files,
<a href="vehicles.zip">vehicles.zip</a> and <a href="non-vehicles.zip">non-vehicles.zip</a>, which as the names suggest
contained images of vehicles and things that are not vehicles.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">img1</span> = mpimg.imread(<span style="color: #e9b96e;">"vehicles/GTI_MiddleClose/image0000.png"</span>)
<span style="color: #fcaf3e;">img2</span> = mpimg.imread(<span style="color: #e9b96e;">"non-vehicles/GTI/image1.png"</span>)
<span style="color: #fcaf3e;">fig</span> = plt.figure()
plt.subplot(121)
plt.imshow(img1)
plt.title(<span style="color: #e9b96e;">'Vehicle'</span>)
plt.subplot(122)
plt.imshow(img2)
plt.title(<span style="color: #e9b96e;">'Non-Vehicle'</span>)
fig.tight_layout()
plt.savefig(<span style="color: #e9b96e;">"output_images/car-examples.png"</span>)
</pre>
</div>

<p>
Here is a side-by-side comparison of a vehicle image and a
non-vehicle image, drawn from our training sample.
</p>


<div class="figure">
<p><img src="output_images/car-examples.png" alt="car-examples.png" />
</p>
</div>

<p>
The size of each image is 64 x 64 pixels, and the vehicle and
non-vehicle images are contained (after unpacking the archive files)
in directories <code>vehicle</code> and <code>non-vehicle</code>, respectively.  Now,
whatever classifier we use, we have to start by reading in these
images one way or another.  Confronted with tasks like this, I like
to compose small functions based on Python <a href="http://davidaventimiglia.com/python_generators.html">generators</a>, so first I
define a handful of useful utility functions.  
</p>

<dl class="org-dl">
<dt> feed </dt><dd>generator function over a <a href="https://docs.python.org/2/library/glob.html">glob</a>, which maps a value <code>y</code> to
each filename that matches <code>pattern</code>, yielding <a href="https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences">tuples</a>
</dd>
<dt> shuffle </dt><dd>list-builder function over a sequence of tuples, which
<a href="https://www.merriam-webster.com/dictionary/reify">reifies</a> it into a randomized list
</dd>
<dt> scale </dt><dd>non-generator function, which scales the values in an
array, either by the maximum value in the array or by a
supplied parameter <code>maxval</code>
</dd>
<dt> load </dt><dd>generator function over a sequence of <a href="https://en.wikipedia.org/wiki/Ordered_pair">ordered pairs</a> in
which the first element is an image filename and the
second is any value (perhaps provided by the <code>feed</code>
function above), which loads the image files into NumPy
arrays
</dd>
<dt> flip </dt><dd>generator function over a sequence of ordered pairs in
which the first element is a NumPy array and the second is
any value, which "flips" the array horizontally (i.e.,
across a vertical axis) and producing a mirror image
</dd>
<dt> mirror </dt><dd>generator function over a sequence of ordered pairs as
in <code>flip</code>, but which yields first the entire sequence
unchanged and then the entire sequence again but with the
images flipped
</dd>
</dl>

<p>
These are implemented as one-liners.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">feed</span> = <span style="color: #b4fa70;">lambda</span> pattern, y: ((f, y) <span style="color: #b4fa70;">for</span> f <span style="color: #b4fa70;">in</span> glob(pattern))
<span style="color: #fcaf3e;">shuffle</span> = <span style="color: #b4fa70;">lambda</span> l: sample(l, <span style="color: #e090d7;">len</span>(l))
<span style="color: #fcaf3e;">scale</span> = <span style="color: #b4fa70;">lambda</span> <span style="color: #fcaf3e;">img</span>,<span style="color: #fcaf3e;">maxval</span>=<span style="color: #e9b2e3;">None</span>: (img/np.<span style="color: #e090d7;">max</span>(img)*255).astype(np.uint8) <span style="color: #b4fa70;">if</span> maxval==<span style="color: #e9b2e3;">None</span> <span style="color: #b4fa70;">else</span> (img/maxval*255).astype(np.uint8)
<span style="color: #fcaf3e;">load</span> = <span style="color: #b4fa70;">lambda</span> g: ((mpimg.imread(x[0]),x[1]) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> g)
<span style="color: #fcaf3e;">flip</span> = <span style="color: #b4fa70;">lambda</span> g: ((x[0][:,::-1,:],x[1]) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> g)
<span style="color: #fcaf3e;">mirror</span> = <span style="color: #b4fa70;">lambda</span> g: chain(g, flip(g))
</pre>
</div>

<p>
When composed together, these functions provide a generator that
<a href="https://en.wikipedia.org/wiki/Lazy_loading">lazily loads</a> training images in random order, twice: first
unflipped, and second flipped.  This serves several related
purposes.  First, randomizing data for training purposes is a
best-practice in Machine Learning.  Second, it effectively doubles
the size of our training set.  Third, we anticipate encountering
vehicles on the road from any angle, where the vehicles themselves
are inherently symmetric across a vertical plane running
longitudinally down the length of the car.
</p>

<p>
Before we can use this generator, however, we need something to use
it on.  Let's define our functions for extracting features and for
creating our classifiers.
</p>

<p>
First, the <code>extract_features</code> function transforms a given image to a
target color space, performs HOG feature extraction on a target
color channel, then <a href="http://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range">scales</a> the features.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">extract_features</span>(img,
                     colorspace=cv2.COLOR_RGB2HSV,
                     channel=2,
                     orient=9,
                     pix_per_cell=8,
                     cell_per_block=4,
                     transform_sqrt=<span style="color: #e9b2e3;">False</span>,
                     feature_vec=<span style="color: #e9b2e3;">True</span>):
    <span style="color: #fcaf3e;">img</span> = scale(img)
    <span style="color: #fcaf3e;">X</span> = np.array([])
    <span style="color: #fcaf3e;">X</span> = np.append(X,
                  hog(cv2.cvtColor(img, colorspace)[:,:,channel],
                      orient,
                      (pix_per_cell,pix_per_cell),
                      (cell_per_block,cell_per_block),
                      transform_sqrt = transform_sqrt,
                      feature_vector = feature_vec))
    <span style="color: #fcaf3e;">s</span> = StandardScaler().fit(X)
    <span style="color: #b4fa70;">return</span> s.transform(X)
</pre>
</div>

<p>
Note that many of the parameters are supplied with default values.
That is no accident.  The values given above, and repeated here, are
the ones used throughout this project, and were obtained through
experimentation by trial-and-error.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Parameter</th>
<th scope="col" class="left">Value</th>
<th scope="col" class="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left"><code>colorspace</code></td>
<td class="left"><code>COLOR_RGB2HSV</code></td>
<td class="left">target color space</td>
</tr>

<tr>
<td class="left"><code>channel</code></td>
<td class="left"><code>2</code></td>
<td class="left">target color channel</td>
</tr>

<tr>
<td class="left"><code>orient</code></td>
<td class="left"><code>9</code></td>
<td class="left">HOG orientation bins</td>
</tr>

<tr>
<td class="left"><code>pix_per_cell</code></td>
<td class="left"><code>8</code></td>
<td class="left">pixels per HOG cell</td>
</tr>

<tr>
<td class="left"><code>cell_per_block</code></td>
<td class="left"><code>4</code></td>
<td class="left">cells per HOG block</td>
</tr>

<tr>
<td class="left"><code>transform_sqrt</code></td>
<td class="left"><code>False</code></td>
<td class="left">scale values by <code>math.sqrt</code></td>
</tr>

<tr>
<td class="left"><code>feature_vec</code></td>
<td class="left"><code>True</code></td>
<td class="left">return feature vector</td>
</tr>
</tbody>
</table>

<p>
Next, the <code>get_classifier</code> function returns a function which is
itself a trained classifier.  Parameters control whether or not to
train the classifier anew or to load a pre-trained classifier from a
file, and what the training/test set split should be when training a
new one.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_classifier</span>(<span style="color: #e090d7;">reload</span>=<span style="color: #e9b2e3;">False</span>,test_size=0.2):
    <span style="color: #b4fa70;">if</span> <span style="color: #e090d7;">reload</span>:
<span id="coderef-compose1" class="coderef-off">        <span style="color: #fcaf3e;">samples</span> = <span style="color: #e090d7;">list</span>(chain(feed(<span style="color: #e9b96e;">"vehicles/**/*.png"</span>,1),feed(<span style="color: #e9b96e;">"non-vehicles/**/*.png"</span>,0)))  <span style="color: #73d216;">#</span></span>
<span id="coderef-compose2" class="coderef-off">        <span style="color: #fcaf3e;">data</span> = cycle(mirror(load(shuffle(samples))))                                        <span style="color: #73d216;">#</span></span>
        <span style="color: #fcaf3e;">X_train</span>,<span style="color: #fcaf3e;">X_test</span>,<span style="color: #fcaf3e;">y_train</span>,<span style="color: #fcaf3e;">y_test</span> = train_test_split(*<span style="color: #e090d7;">zip</span>(*((extract_features(s[0]), s[1]) <span style="color: #b4fa70;">for</span> s <span style="color: #b4fa70;">in</span> islice(data, <span style="color: #e090d7;">len</span>(samples)))), test_size=test_size, random_state=np.random.randint(0, 100))
        <span style="color: #fcaf3e;">svc</span> = LinearSVC()
        svc.fit(X_train, y_train)
        pickle.dump(svc, <span style="color: #e090d7;">open</span>(<span style="color: #e9b96e;">"save.p"</span>,<span style="color: #e9b96e;">"wb"</span>))
        <span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">'Test Accuracy of SVC = '</span>, <span style="color: #e090d7;">round</span>(classifier.score(X_test, y_test), 4))
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #fcaf3e;">svc</span> = pickle.load(<span style="color: #e090d7;">open</span>(<span style="color: #e9b96e;">"save.p"</span>, <span style="color: #e9b96e;">"rb"</span>))
    <span style="color: #b4fa70;">return</span> svc
</pre>
</div>

<p>
Note the use of our composable utility functions to load the data
<a href="#coderef-compose1"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-compose1');" onmouseout="CodeHighlightOff(this, 'coderef-compose1');">here</a> and <a href="#coderef-compose2"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-compose2');" onmouseout="CodeHighlightOff(this, 'coderef-compose2');">here</a>.  Note also that there are a variety of classifiers we
could use.
</p>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine (SVM)</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a>
</li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>
</li>
</ul>

<p>
I was prepared to experiment with each of these, and perhaps with
their combinations.  I started with an SVG, however, and found that
it performed well all on its own.
</p>

<p>
Training a classifier now is as simple as
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #73d216;"># </span><span style="color: #73d216;">classifier = get_classifier(True)</span>
</pre>
</div>

<pre class="example">
Test Accuracy of SVC =  0.9938
</pre>

<p>
while loading a saved classifier is even simpler
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">classifier</span> = get_classifier()
</pre>
</div>

<p>
Saving and re-loading classifiers like in this second command was
very helpful in this project for promoting rapid iteration, because
once I had a classifier I was happy with, I could move onto
subsequent stages in the project without needing to retrain the
classifier every time. 
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Sliding Window Search I</h2>
<div class="outline-text-2" id="text-4">
<p>
I performed the most experimentation on various sliding window
schemes.  Initially, I expended effort on behalf of a single idea:
<i>Can I model vehicle position not in screen coordinates, measured in
pixels, but rather in real-world coordinates, measured in meters?</i>
My strategy was to generate sliding windows on a three-dimensional
(3D) grid whose origin is where the camera is placed and whose units
are meters, and then use geometry to project those windows onto the
screen in pixel coordinates.  This model has these assumptions.
</p>

<ul class="org-ul">
<li>The image plane roughly corresponds to the vehicle's windshield.
</li>
<li>The windshield is approximately 2 meters wide, 1 meter tall, and 2
meters above the road.
</li>
<li>The camera is placed approximately 1 meter behind the windshield's
center, with a line-of-sight (LOS) perpendicular to it.
</li>
<li>The grid coordinates \(\left( x, y, z \right)_{grid}\) correspond to
the horizontal position across the road, the vertical position
above the road, and the longitudinal position down the road.
</li>
<li>Positive \(x\) values are to the right, negative \(x\) values are to
the left, and \(x_{grid} \in \left[ -15, 15 \right]\).
</li>
<li>Negative \(y\) values are below the camera, and \(y_{grid} \in \left[
    -2, 0 \right]\).
</li>
<li>\(z \lt 1\) values are inside the car, and \(z_{grid} \in \left[ 10,
    100 \right]\).
</li>
</ul>

<p>
These assumptions determine the geometry of the problem and set its
physical scale, with a field-of-view (FOV) of 90Â°, and allow us to
create sliding windows as described above.  In principle, vehicle
detections on image patches can then be assigned real-world
coordinates \((x, y, z)\), or at least road coordinates \((x,
  z)_{y=0}\), a real-world "heat map" can be built up, and then
individual vehicles can be identified either with conventional
thresholds + <a href="http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html">labeling</a>, with a <a href="https://en.wikipedia.org/wiki/Blob_detection">Laplace-of-Gaussian</a> technique, or
with <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.vq.html"><i>k</i>-means clustering</a>.
</p>

<p>
The following coordinate conversion functions support the geometrical model outlined
above.
</p>

<dl class="org-dl">
<dt> crt2cyl </dt><dd>Cartesian-to-cylindrical
</dd>
<dt> cyl2crt </dt><dd>cylindrical-to-Cartesian
</dd>
<dt> cyl2sph </dt><dd>cylindrical-to-spherical
</dd>
<dt> sph2cyl </dt><dd>spherical-to-cylindrical
</dd>
<dt> crt2sph </dt><dd>Cartesian-to-spherical
</dd>
<dt> sph2crt </dt><dd>spherical-to-Cartesian
</dd>
</dl>

<p>
These are implemented as one-liners
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">crt2cyl</span> = <span style="color: #b4fa70;">lambda</span> x,y,z: (math.sqrt(x**2+y**2), math.atan2(y,x), z)
<span style="color: #fcaf3e;">cyl2crt</span> = <span style="color: #b4fa70;">lambda</span> rho,phi,z: (rho*math.cos(phi), rho*math.sin(phi), z)
<span style="color: #fcaf3e;">cyl2sph</span> = <span style="color: #b4fa70;">lambda</span> rho,phi,z: (math.sqrt(rho**2+z**2), math.atan2(rho, z), phi)
<span style="color: #fcaf3e;">sph2cyl</span> = <span style="color: #b4fa70;">lambda</span> r,theta,phi: (r*math.sin(theta), phi, r*math.cos(theta))
<span style="color: #fcaf3e;">crt2sph</span> = <span style="color: #b4fa70;">lambda</span> x,y,z: (math.sqrt(x**2+y**2+z**2), math.acos(z/math.sqrt(x**2+y**2+z**2)), math.atan2(y,x))
<span style="color: #fcaf3e;">sph2crt</span> = <span style="color: #b4fa70;">lambda</span> r,theta,phi: (r*math.sin(theta)*math.cos(phi), r*math.sin(theta)*math.sin(phi), r*math.cos(theta))
</pre>
</div>

<p>
The <code>get_window</code> function computes from \((x,y,z)\) a "window", which
is a list of tuples wherein the first two provide the corners of its
"bounding box" and the last provides the coordinates of its center.
Note that it also takes <code>height</code> and <code>width</code> parameters for the
physical size (in meters) of the window, as well as a <code>horizon</code>
parameter, which is the fraction of the image plane (from below) at
which the horizon appears.  The default value of <code>0.5</code> corresponds
to the middle.  Finally, like many of my functions it takes a NumPy
image array parameter, <code>img</code>, which is mainly for extracting the
size and shape of the image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_window</span>(img, x, y, z, horizon=0.5, width=2, height=2):
    <span style="color: #fcaf3e;">d</span> = 1
    <span style="color: #fcaf3e;">r</span>,<span style="color: #fcaf3e;">theta</span>,<span style="color: #fcaf3e;">phi</span> = crt2sph(x,y,z)
    <span style="color: #fcaf3e;">rho2</span> = d*math.tan(theta)
    <span style="color: #fcaf3e;">x2</span>,<span style="color: #fcaf3e;">y2</span> = (rho2*math.cos(phi),rho2*math.sin(phi))
    <span style="color: #fcaf3e;">center</span> = (<span style="color: #e090d7;">int</span>(img.shape[1]*0.5+x2*img.shape[1]//2),
              <span style="color: #e090d7;">int</span>(img.shape[0]*(1-horizon)-y2*img.shape[1]//2))
    <span style="color: #fcaf3e;">scale</span> = img.shape[1]//2
    <span style="color: #fcaf3e;">dx</span> = <span style="color: #e090d7;">int</span>(width/2*scale/z)
    <span style="color: #fcaf3e;">dy</span> = <span style="color: #e090d7;">int</span>(height/2*scale/z)
    <span style="color: #fcaf3e;">window</span> = [(center[0]-dx,center[1]-dy), (center[0]+dx,center[1]+dy)] + [(x,y,z)]
    <span style="color: #b4fa70;">return</span> window
</pre>
</div>

<p>
Next, the <code>draw_window</code> function annotates an image <code>img</code> with the
window <code>bbox</code>.  This does not factor into the actual vehicle
detection, of course, but the visualization is valuable for
understanding how the video processing pipeline ultimately is
working.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">draw_window</span>(img, bbox, color=(0,0,255), thick=3):
    cv2.rectangle(img, bbox[0], bbox[1], color, thick)
    <span style="color: #b4fa70;">return</span> img
</pre>
</div>

<p>
For example, first we draw a window box that roughly corresponds to
the windshield itself, using a test image from the lecture notes.
The windshield's center is at real-world coordinates
\((x,y,z)_{windshield} = (0,0,1)\).
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
draw_window(image, get_window(image, 0, 0.0, 1, horizon=0.5, width=2, height=1))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/windshield.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/windshield.png" alt="windshield.png" width="800px" />
</p>
</div>

<p>
Next, we draw window boxes around a few of the cars in the image.
Note that here we are eschewing the default value of <code>horizon</code> in
favor of <code>0.28</code>, given the peculiar tilt the camera seems to have in
this image.  That value, like the real-world vehicle coordinates
\((x,y,z)_i\), were obtained by hand through trial-and-error.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
draw_window(image, get_window(image, 4.1, -1.0, 8, horizon=0.28))
draw_window(image, get_window(image, -10.5, -1.0, 22, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 32, horizon=0.28))
draw_window(image, get_window(image, -0.8, -1.0, 35, horizon=0.28))
draw_window(image, get_window(image, 3, -1.0, 55, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 55, horizon=0.28))
draw_window(image, get_window(image, -6.1, -1.0, 70, horizon=0.28))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/bbox-example-image-test.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/bbox-example-image-test.png" alt="bbox-example-image-test.png" width="800px" />
</p>
</div>

<p>
In order to help visualize the geometry further, we animate a
handful of windows receding into the distance.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">zooming_windows</span>(img):
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">make_frame</span>(t):
        <span style="color: #fcaf3e;">frame</span> = np.copy(img)
        <span style="color: #fcaf3e;">z</span> = 2**(t % 5)*5
        draw_window(frame, get_window(frame,-10.5,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,-6.1,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,-0.8,-1.0,z,horizon=0.28))
        draw_window(frame, get_window(frame,4.1,-1.0,z,horizon=0.28))
        cv2.putText(frame, <span style="color: #e9b96e;">"z: %.2f m"</span> % z, (50,50), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2)
        <span style="color: #b4fa70;">return</span> frame
    <span style="color: #b4fa70;">return</span> make_frame
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">clip</span> = VideoClip(zooming_windows(mpimg.imread(<span style="color: #e9b96e;">'bbox-example-image.jpg'</span>)), duration=5)
clip.write_videofile(<span style="color: #e9b96e;">"output_images/zooming-windows.mp4"</span>, fps=25)
</pre>
</div>

<iframe width="800" height="450" src="https://www.youtube.com/embed/lqp9rOSPVrc" frameborder="0" allowfullscreen></iframe>

<p>
This is just for visualization.  For vehicle detection, a denser
grid should be used, and we raster the windows horizontally as they
ratchet down-range.  We also confine the windows to a horizontal
plane, at \(z = -1\).  But, because this sliding window and other ones
like it actually will be used in the vehicle-detection
video-processing pipeline, it is worthwhile to remove windows that
exceed the image boundary.  That is the purpose of the <code>clip_window</code>
function.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">clip_window</span>(x, box):
    <span style="color: #b4fa70;">return</span> <span style="color: #e090d7;">sum</span>([box[0]&lt;=x[0][0]&lt;=box[1],
                box[0]&lt;=x[1][0]&lt;=box[1],
                box[2]&lt;=x[0][1]&lt;=box[3],
                box[2]&lt;=x[1][1]&lt;=box[3]])==4
</pre>
</div>


<p>
Since our strategy will be to write functions to produce "grids"
that can be used both for visualization and for vehicle-detection,
we refactor much of the animated visualization into a new function,
<code>get_frame_maker</code>.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_frame_maker</span>(img, grid):
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">make_frame</span>(t):
        <span style="color: #fcaf3e;">frame</span> = np.copy(img)
        draw_window(frame, grid.__next__()[:2], color=(0,255,255))
        <span style="color: #b4fa70;">return</span> frame
    <span style="color: #b4fa70;">return</span> make_frame
</pre>
</div>

<p>
With these tools, first we define a "sparse grid"
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">sparse_scan</span>(img):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:2,-1.0:0:2,3:7:1]
    <span style="color: #fcaf3e;">grid</span>[2,]=2**grid[2,]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, (0, img.shape[1], (img.shape[0]//2), img.shape[0])), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<p>
visualize its 40 windows
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), sparse_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/sparse-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
52
</pre>


<div class="figure">
<p><img src="output_images/sparse-scan.png" alt="sparse-scan.png" width="800px" />
</p>
</div>

<p>
and then animate them.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">clip</span> = VideoClip(get_frame_maker(image, cycle(sparse_scan(image))), duration=10)
clip.write_videofile(<span style="color: #e9b96e;">"output_images/sparse-scan.mp4"</span>, fps=25)
</pre>
</div>

<iframe width="800" height="450" src="https://www.youtube.com/embed/Vn1HxPRd2W0" frameborder="0" allowfullscreen></iframe>

<p>
We can also define a "dense grid" with more windows, scanning the
roadway with finer resolution in the \(x\) and \(z\) directions.  We
skip the animation this time, as it is rather boring.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">dense_scan</span>(img, h=2,w=2):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:0.5,-1.0:0:2,10:100:2]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28, height=h, width=w)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, (0, img.shape[1], (img.shape[0]//2), img.shape[0])), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<p>
When produce the grid image, note that it has 2600+ windows!  That
probably is excessive and would slow down video processing.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), dense_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/dense-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
2653
</pre>


<div class="figure">
<p><img src="output_images/dense-scan.png" alt="dense-scan.png" width="800px" />
</p>
</div>

<p>
The sparse scan above probably is too sparse, but one way we can
reduce the number of windows would be to search the perimeter of the
road, where new cars are likely to come on-stage.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">perimeter_scan</span>(img):
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[-15:15:0.5,-1.0:0:2,10:100:2]
    <span style="color: #fcaf3e;">grid</span> = grid.T.reshape(-1,3)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: <span style="color: #b4fa70;">not</span> (-4&lt;=x[0]&lt;=4 <span style="color: #b4fa70;">and</span> 5&lt;=x[2]&lt;=40), grid))
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], horizon=0.28)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, (0, img.shape[1], (img.shape[0]//2), img.shape[0])), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"bbox-example-image.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), perimeter_scan(image)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/perimeter-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
2381
</pre>

<p>
Sadly, this barely makes a dent in reducing the number of windows.  
</p>


<div class="figure">
<p><img src="output_images/perimeter-scan.png" alt="perimeter-scan.png" width="800px" />
</p>
</div>

<p>
In order to make headway, a simple choice is just to stick with the
dense grid, perform vehicle detections with it against a test image,
and gauge its performance.
</p>

<p>
To do that, we need a function <code>get_patches</code>, that takes a <i>window</i>,
which again is mainly a bounding-box (with pixel dimensions) into a
<i>patch</i>, which is a NumPy image sub-array taken from a larger image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_patches</span>(img, grid, size=(64,64)):
    <span style="color: #b4fa70;">return</span> ((cv2.resize(img[window[0][1]:window[1][1],
                            window[0][0]:window[1][0]],size),window) <span style="color: #b4fa70;">for</span> window <span style="color: #b4fa70;">in</span> grid)
</pre>
</div>

<p>
Armed with that function, next we just map our classifier over all
of the window patches on an image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">process</span>(x):
    <span style="color: #b4fa70;">return</span> (classifier.predict(extract_features(x[0]))[0],x[1])
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">results</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(process, get_patches(image, dense_scan(image))))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(results))
</pre>
</div>

<pre class="example">
2653
</pre>

<p>
To visualize where vehicle detections have occurred on our dense grid
over the road, we filter the processed results that have a value
greater than 1 (i.e., a detection has occurred for that window patch)
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">_</span>,<span style="color: #fcaf3e;">r</span> = <span style="color: #e090d7;">zip</span>(*<span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: x[0]&gt;0, results))
<span style="color: #fcaf3e;">_</span>,<span style="color: #fcaf3e;">_</span>,<span style="color: #fcaf3e;">cen</span>,<span style="color: #fcaf3e;">_</span> = <span style="color: #e090d7;">zip</span>(*r)
<span style="color: #fcaf3e;">x</span>,<span style="color: #fcaf3e;">y</span>,<span style="color: #fcaf3e;">z</span> = <span style="color: #e090d7;">zip</span>(*cen)
plt.scatter(x,z,s=50,c=y)
</pre>
</div>

<pre class="example">
&gt;&gt;&gt; &gt;&gt;&gt; &lt;matplotlib.collections.PathCollection object at 0x7f8fb84b2710&gt;
</pre>


<div class="figure">
<p><img src="output_images/figure_4.png" alt="figure_4.png" width="800px" />
</p>
</div>

<p>
These results are interesting and suggestive.  The contiguous
regions of detections presumably correspond to vehicles, with their
2D location associated with the "center" of each island.  However,
projection effects seem to elongate the detected regions with a
strong, pronounced radial pattern, which could be problematic.
Perhaps with suitable thresholding, a technique as simple as the
<code>label</code> function would be sufficient for picking out the cars.  On
the other hand, we might need more sophisticated techniques, such as
<a href="https://en.wikipedia.org/wiki/Blob_detection">Laplace-of-Gaussian</a> or with <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.vq.html"><i>k</i>-means clustering</a>.  This is an
intriguing direction of inquiry to pursue in further studies,
however in this one I found that I was running out of time.  
</p>

<p>
So, I switched gears to a more traditional sliding windows approach.
</p>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Sliding Window Search II</h2>
<div class="outline-text-2" id="text-5">
<p>
To refresh the reader, a more traditional sliding windows approach
models a grid of windows and their image patches not in real-world
3D physical space, but in 2D image space.  This involves trade-offs.
On the one hand, we give up a straightforward 3D interpretation of a
vehicle detection event.  In principle, we could still recover
distance information by deprojecting the window (the reverse of our
operation above), but at the expense of greater complication.  On
the other hand, we gain with this trade-off a simpler implementation
that already has a proven track-record.
</p>

<p>
We can reuse much of our other code, though, since we just need to
define functions to produce grids that obey whatever selection
functions we desire.
</p>

<p>
First up is a simple "image-plane scan", which carpets the image
plane in a uniform grid of windows at various fixed scales.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">image_plane_scan</span>(img,ny,overlap,scale):
    <span style="color: #fcaf3e;">size</span> = <span style="color: #e090d7;">int</span>(img.shape[0]//ny)//scale
    <span style="color: #fcaf3e;">delta</span> = <span style="color: #e090d7;">int</span>(size*(1-overlap))
    <span style="color: #fcaf3e;">box1</span> = (0,
            img.shape[1],
            (img.shape[0]-img.shape[0]//scale)//2,
            img.shape[0] - (img.shape[0]-img.shape[0]//scale)//2)
    <span style="color: #fcaf3e;">box2</span> = (0,
            img.shape[1],
            (img.shape[0]//2),
            img.shape[0])
    <span style="color: #fcaf3e;">grid</span> = np.mgrid[0:img.shape[1]:delta,
                    img.shape[0]:-delta:-delta].T.reshape(-1,2)
    <span style="color: #fcaf3e;">grid</span> = ([(c[0],c[1]), (c[0]+size,c[1]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box1), grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box2), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>()
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       image_plane_scan(image,4,0.50,1),
                       image_plane_scan(image,4,0.50,2),
                       image_plane_scan(image,4,0.50,3)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/imageplane-scan.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/imageplane-scan.png" alt="imageplane-scan.png" width="800px" />
</p>
</div>

<p>
This produces 1400+ images, which highlights a persistent problem I
grappled with.  There is an inherent trade-off between the accuracy
of a dense window sample, and the performance of a sparse window sample.
</p>

<p>
A conjecture I had to help ease the tension between these two poles
was to relax the constraint of a regular grid of windows in favor of
a random scattering of windows.  One of the reasons the window count
soared with a regular grid was the overlap; a high degree of overlap
(&gt;50%) was needed for higher spatial resolution of detected vehicle
locations, but the number of windows is essentially quadratic in the
degree of overlap.  However, the stochastic behavior of an irregular
random sampling of windows means that a higher spatial resolution
can be achieved in an economy of window patches.  
</p>

<p>
The trade-offs here, however, are two-fold.  First, we no longer can
pre-compute the grid, but instead must compute a new random ensemble
of windows for each video frame.  In the testing that I did, this
proved to be of little concern; the Python profiler, and experience
as well, showed that the grid computation time was relatively
trivial.  The bulk of the time was spent on feature extraction and
classification for each window patch, a task that obviously cannot
be precomputed irrespective of the grid strategy.
</p>

<p>
Second, since any one <i>particular</i> frame is treated with a
relatively sparse (but now random) irregular grid of windows, this
intensifies the need for integrating the signal over multiple frames
(a task we anticipated in any case).  Consequently, we lose
resolution in the time domain.  While that could be a problem for
fast-moving vehicles, it was not for the relatively slow relative
velocity of the vehicles in our project video.
</p>

<p>
My first version of a random scan uses a region-of-interest mask
that selects out a trapezoidal region covering just the border of
the road.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">region_of_interest</span>(img, vertices):
    <span style="color: #fcaf3e;">mask</span> = np.zeros_like(img)   
    <span style="color: #b4fa70;">if</span> <span style="color: #e090d7;">len</span>(img.shape) &gt; 2:
        <span style="color: #fcaf3e;">channel_count</span> = img.shape[2]
        <span style="color: #fcaf3e;">ignore_mask_color</span> = (255,) * channel_count
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #fcaf3e;">ignore_mask_color</span> = 255
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    <span style="color: #fcaf3e;">masked_image</span> = cv2.bitwise_and(img, mask)
    <span style="color: #b4fa70;">return</span> masked_image
</pre>
</div>

<p>
The actual function <code>random_scan</code> takes an image <code>img</code> (again, just
for the size information), and a window size.  Since these we are
now operating in the pixel coordinates of the image plane rather
than in the physical coordinates of the real world, the window size
is taken just in pixels.  This function works by thresholding a
random array.  It is a somewhat elegant technique, but is
inefficient and <i>slowww</i>.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan</span>(img,size):
    <span style="color: #fcaf3e;">x</span> = np.random.rand(*img.shape[:2])
    <span style="color: #fcaf3e;">x</span>[x&lt;0.999] = 0
    <span style="color: #fcaf3e;">x</span> = scale(np.ceil(x))
    <span style="color: #fcaf3e;">x</span> = region_of_interest(x, np.array([[[0, 0.5*image.shape[0]],
                           [image.shape[1], 0.5*image.shape[0]],
                           [image.shape[1], image.shape[0]],
                           [(1-2/6)*image.shape[1], 0.5*image.shape[0]],
                           [(2/6)*image.shape[1], 0.5*image.shape[0]],
                           [0, image.shape[0]]]]).astype(<span style="color: #e9b96e;">'int'</span>))
    <span style="color: #fcaf3e;">x</span> = np.dstack(np.nonzero(x))
    <span style="color: #fcaf3e;">s</span> = np.random.choice(2**np.arange(4), <span style="color: #e090d7;">len</span>(x[0]))
    <span style="color: #fcaf3e;">grid</span> = ([(c[1],c[0]),
             (c[1]+size,c[0]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> x[0])
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>()
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan(image,180),
                       random_scan(image,90),
                       random_scan(image,60)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan1.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/random-scan1.png" alt="random-scan1.png" width="800px" />
</p>
</div>

<p>
The next random grid function <code>random_scan2</code>, uses a slightly
less-elegant approach, but is noticeably faster.  Aside from
confining the window to the bottom half of the image, however, it
does not use a region-of-interest mask.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan2</span>(img,size,num=100):
    <span style="color: #fcaf3e;">x</span> = np.random.rand(num,2)
    <span style="color: #fcaf3e;">x</span>[:,0]*=image.shape[1]
    <span style="color: #fcaf3e;">x</span>[:,1]*=image.shape[1]
    <span style="color: #fcaf3e;">x</span> = x.astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #fcaf3e;">x</span> = x[x[:,1]&lt;image.shape[0]]
    <span style="color: #fcaf3e;">x</span> = x[x[:,1]&gt;=image.shape[0]//2]
    <span style="color: #fcaf3e;">box</span> = (0,img.shape[1],(img.shape[0]//2),650)
    <span style="color: #fcaf3e;">grid</span> = ([(c[0],c[1]),
             (c[0]+size,c[1]+size)] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> x)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan2(image,256,1000),
                       random_scan2(image,128,1000),
                       random_scan2(image,64,1000)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan2.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... Number of windows: 311
</pre>


<div class="figure">
<p><img src="output_images/random-scan2.png" alt="random-scan2.png" width="800px" />
</p>
</div>

<p>
The next random scanner I tried worked in polar (pixel) coordinates
so as to achieve a masking affect, that concentrates windows on the
road borders where vehicles are most likely to appear.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan3</span>(img,size,num=100,minr=<span style="color: #e9b2e3;">None</span>,maxr=<span style="color: #e9b2e3;">None</span>,mintheta=<span style="color: #e9b2e3;">None</span>,maxtheta=<span style="color: #e9b2e3;">None</span>,center=<span style="color: #e9b2e3;">None</span>,scale=<span style="color: #e9b2e3;">True</span>):
    <span style="color: #b4fa70;">if</span> center==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">center</span> = <span style="color: #e090d7;">tuple</span>(np.array(image.shape[:2][::-1])//2)
    <span style="color: #fcaf3e;">polar</span> = np.random.rand(num,2)
    <span style="color: #fcaf3e;">polar</span>[:,0]*=image.shape[1]
    <span style="color: #fcaf3e;">polar</span>[:,1]*=math.pi*2
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> minr==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,0]&gt;=minr]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> maxr==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,0]&lt;maxr]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> mintheta==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,1]&gt;=0]
    <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">not</span> maxtheta==<span style="color: #e9b2e3;">None</span>:
        <span style="color: #fcaf3e;">polar</span> = polar[polar[:,1]&lt;maxtheta]
    <span style="color: #b4fa70;">if</span> scale:
        <span style="color: #fcaf3e;">s</span> = (size//2*polar[:,0]/image.shape[1]).astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #b4fa70;">else</span>:
        <span style="color: #b4fa70;">try</span>:
            <span style="color: #fcaf3e;">dist</span> = <span style="color: #e090d7;">int</span>(math.sqrt(<span style="color: #e090d7;">sum</span>([(center[0]-image.shape[1]//2)**2,
                                      (center[1]-image.shape[0]//2)**2])))
            <span style="color: #fcaf3e;">s</span> = [<span style="color: #e090d7;">int</span>(size*(dist/(image.shape[1]//2)))]*<span style="color: #e090d7;">len</span>(polar)
        <span style="color: #b4fa70;">except</span>:
            pdb.set_trace()
    <span style="color: #fcaf3e;">x</span>,<span style="color: #fcaf3e;">y</span>=<span style="color: #e090d7;">zip</span>(*np.dstack((center[0]+polar[:,0]*np.cos(polar[:,1]),
                        center[1]+polar[:,0]*np.sin(polar[:,1]))).astype(<span style="color: #e9b96e;">'int'</span>)[0])
    <span style="color: #fcaf3e;">grid</span> = ([(c[0]-c[2],c[1]-c[2]), (c[0]+c[2],c[1]+c[2])] <span style="color: #b4fa70;">for</span> c <span style="color: #b4fa70;">in</span> <span style="color: #e090d7;">zip</span>(x,y,s))
    <span style="color: #fcaf3e;">box</span> = (0,img.shape[1],(0),670)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, box), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">"Number of windows: %s"</span> %
      <span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]),
                   chain(
                       random_scan3(image,image.shape[1]//4,
                                    3000,
                                    minr=image.shape[0]//3,
                                    mintheta=0,
                                    maxtheta=math.pi)
                   )))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan3.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
... ... ... ... ... ... ... ... Number of windows: 199
</pre>


<div class="figure">
<p><img src="output_images/random-scan3.png" alt="random-scan3.png" width="800px" />
</p>
</div>

<p>
This produces an interesting pattern, but I was not comfortable
peculiar way the windows are scaled to different sizes, so I wrote
yet another grid window function <code>random_scan4</code>, which is a bit of a
hybrid.  It actually re-uses the 3D model described above in
<b>Sliding Window Search I</b>.  Windows are defined in a 3D volume which
covers the road from left to right, from the car to the horizon, and
from the camera level down to the road.  I.e., it is like a long,
thick "ribbon", within which windows are randomly sampled.  As
above, we are back in physical space for window sizes, rather than
in pixel space.  Finally, the physical space windows are projected
back onto the image plane to give us a grid window in pixel-space.
In fact, this is almost exactly as we did in the earlier section.
The main differences are:
</p>

<ol class="org-ol">
<li>We discard the 3D window location information after projecting it
to a grid window on the image plane.
</li>
<li>Window locations are randomly drawn from the 3D volume described
above, rather than laid out in a regular array.
</li>
</ol>

<p>
This is implemented in the <code>random_scan4</code> function.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan4</span>(img,size,num=100,width=25,left=-12.5):
    <span style="color: #fcaf3e;">grid</span> = np.random.rand(num,3)
    <span style="color: #fcaf3e;">grid</span>[:,0]*=width
    <span style="color: #fcaf3e;">grid</span>[:,1]*=2
    <span style="color: #fcaf3e;">grid</span>[:,1]-=2
    <span style="color: #fcaf3e;">grid</span>[:,2]*=40
    <span style="color: #fcaf3e;">grid</span>[:,0]+=left
    <span style="color: #fcaf3e;">grid</span>[:,1]-=4
    <span style="color: #fcaf3e;">grid</span>[:,2]+=5
    <span style="color: #fcaf3e;">grid</span> = grid.astype(<span style="color: #e9b96e;">'int'</span>)
    <span style="color: #fcaf3e;">grid</span> = (get_window(img,x[0],x[1],x[2], height=4, width=4)+[x] <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> grid)
    <span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: clip_window(x, (0, img.shape[1], (img.shape[0]//2), img.shape[0])), grid)
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), random_scan4(image,2,1000)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan4.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
813
</pre>


<div class="figure">
<p><img src="output_images/random-scan4.png" alt="random-scan4.png" width="800px" />
</p>
</div>

<p>
Note that the above function takes parameters <code>width</code> and <code>left</code>
which set the width of the "ribbon" volume, and its left edge (in
meters).  We can easily combine a couple calls to this grid
generating function with judicious parameter choices in order to
archive interesting search patterns.  For instance, in
<code>random_scan5</code>, we superimpose two ribbons, one on the left, and one
on the right, in order just to search the road borders.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">random_scan5</span>(img,size,num=100):
    <span style="color: #fcaf3e;">grid</span> = chain(random_scan4(img,size,num//2,width=20,left=-30),
                 random_scan4(img,size,num//2,width=20,left=+10))
    <span style="color: #b4fa70;">return</span> grid
</pre>
</div>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #b4fa70;">print</span>(<span style="color: #e090d7;">len</span>(<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(image, w[:2]), random_scan5(image,2,1000)))))
mpimg.imsave(<span style="color: #e9b96e;">"output_images/random-scan5.png"</span>, image, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>)
</pre>
</div>

<pre class="example">
587
</pre>


<div class="figure">
<p><img src="output_images/random-scan5.png" alt="random-scan5.png" width="800px" />
</p>
</div>

<p>
While we may not want to search in this way in general, since the
void in the middle is a giant "blind spot", as a visualization this
has the nice property of removing foreground windows so that the way
they naturally scale with distance is revealed.  
</p>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Video Implementation</h2>
<div class="outline-text-2" id="text-6">
<p>
With a variety of strategies for searching a video frame for vehicle
detection events, the next major step is to adapt those strategies
into an implementation of a video-processing pipeline.  The
processing pipeline has these major steps.
</p>

<ol class="org-ol">
<li>Get a frame of video.
</li>
<li>Generate a grid of windows using one of the schemes developed
above.
</li>
<li>Using the frame image and the grid of windows, generate a
sequence of patches, which are small (64 x 64 pixel) sub-arrays,
compatible with our classifier.
</li>
<li>Apply the classifier as a stencil over the sequence of patches to
generate a sequence of detection/non-detection events.
</li>
<li>Assign a positive value (1) to the bounding box associated with
each window/patch, and superimpose these detection patches to
create a 2D histogram, which we'll call a "heat map."
</li>
<li>Apply a threshold to the heat map for each frame, by selecting
only those array values that exceed the threshold.
</li>
<li>Use the <a href="http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label"><code>label</code></a> function to identify connected regions in the
thresholded heat map, and associate these with "vehicles."
</li>
<li>Annotate the original frame image with a bounding box for each
vehicle.
</li>
</ol>

<p>
The previous sections were largely the province of Steps 2, 3, and 4
above.  Picking up from there with step 5, we need to build up a
heat map.  The <code>add_heat</code> function does just that.  It takes
single-channel image array <code>heatmap</code> (typically, all zeros) and a
list of window/patches in <code>bbox_list</code>, and builds up the
histogram/heat map.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">add_heat</span>(heatmap, bbox_list):
    <span style="color: #b4fa70;">for</span> box <span style="color: #b4fa70;">in</span> bbox_list:
        heatmap[box[0][1]:box[1][1],
                box[0][0]:box[1][0]] += 1
    <span style="color: #b4fa70;">return</span> heatmap
</pre>
</div>

<p>
For step 6, we add the function <code>apply_threshold</code>, which selects out
of a heatmap image array <code>heat</code> only those array elements that
exceed <code>threshold</code>.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">apply_threshold</span>(heat, threshold):
    <span style="color: #fcaf3e;">heatmap</span> = np.copy(heat)
    <span style="color: #fcaf3e;">heatmap</span>[heatmap &lt;= threshold] = 0
    <span style="color: #b4fa70;">return</span> heatmap
</pre>
</div>

<p>
Steps 7 and 8 are combined in the next function,
<code>draw_labeled_boxes</code>, which takes a multi-channel image array
(typically, the original frame of video) in <code>img</code>, along with
labeled regions in <code>labels</code>, computes the locations and bounding
boxes for detected vehicles, and annotates the frame image with a
box.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">draw_labeled_bboxes</span>(img, labels):
    <span style="color: #b4fa70;">for</span> car_number <span style="color: #b4fa70;">in</span> <span style="color: #e090d7;">range</span>(1, labels[1]+1):
        <span style="color: #fcaf3e;">nonzero</span> = (labels[0] == car_number).nonzero()
        <span style="color: #fcaf3e;">nonzeroy</span> = np.array(nonzero[0])
        <span style="color: #fcaf3e;">nonzerox</span> = np.array(nonzero[1])
        <span style="color: #fcaf3e;">bbox</span> = ((np.<span style="color: #e090d7;">min</span>(nonzerox), np.<span style="color: #e090d7;">min</span>(nonzeroy)),
                (np.<span style="color: #e090d7;">max</span>(nonzerox), np.<span style="color: #e090d7;">max</span>(nonzeroy)))
        <span style="color: #fcaf3e;">center</span> = (<span style="color: #e090d7;">int</span>(np.mean((np.<span style="color: #e090d7;">min</span>(nonzerox), np.<span style="color: #e090d7;">max</span>(nonzerox)))),
                  <span style="color: #e090d7;">int</span>(np.mean((np.<span style="color: #e090d7;">min</span>(nonzeroy), np.<span style="color: #e090d7;">max</span>(nonzeroy)))))
        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)
        cv2.putText(img, <span style="color: #e9b96e;">"Car: %s"</span> % car_number,
                    (bbox[0][0],bbox[0][1]-20),
                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255), 1)
        cv2.putText(img, <span style="color: #e9b96e;">"Center: %s"</span> % (center,),
                    (bbox[0][0],bbox[0][1]-10),
                    cv2.FONT_HERSHEY_DUPLEX, .5, (255,255,255), 1)
    <span style="color: #b4fa70;">return</span> img
</pre>
</div>

<p>
In order to test these functions on a single image, we define a
simple <code>process</code> function, which does feature-extraction and
classification using the <code>extract_features</code> function and the
<code>classifier</code> we trained earlier.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">process</span>(x):
    <span style="color: #b4fa70;">return</span> (classifier.predict(extract_features(x[0]))[0],x[1])
</pre>
</div>

<p>
For this test, we choose as our grid strategy just the simple
<code>image_plane_scan</code> defined above.  Recall that this function covers
an image with regular arrays of overlapping windows at fixed scales,
and that it trades performance for simplicity.  The fact that it
generates many windows and therefore operates slowly is of no
concern for just one image.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #fcaf3e;">grid</span> = <span style="color: #e090d7;">list</span>(chain(
    image_plane_scan(image,4,0.750,1),
    image_plane_scan(image,4,0.750,2),
    image_plane_scan(image,4,0.750,3),
))
<span style="color: #fcaf3e;">results</span> = <span style="color: #e090d7;">map</span>(process, get_patches(image, grid))
<span style="color: #fcaf3e;">image</span> = scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>))
<span style="color: #fcaf3e;">box_list</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> x: x[1][:2], <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: x[0]&gt;0, results)))
<span style="color: #fcaf3e;">heat</span> = np.zeros_like(image[:,:,0]).astype(np.<span style="color: #e090d7;">float</span>)
<span style="color: #fcaf3e;">heat</span> = add_heat(heat,box_list)
<span style="color: #fcaf3e;">heat</span> = apply_threshold(heat,5)
<span style="color: #fcaf3e;">labels</span> = label(heat)
<span style="color: #b4fa70;">print</span>(labels[1], <span style="color: #e9b96e;">'cars found'</span>)
<span style="color: #fcaf3e;">draw_img</span> = draw_labeled_bboxes(np.copy(image), labels)
<span style="color: #fcaf3e;">fig</span> = plt.figure()
plt.subplot(121)
plt.imshow(draw_img)
plt.title(<span style="color: #e9b96e;">'Car Positions'</span>)
plt.subplot(122)
plt.imshow(heat, cmap=<span style="color: #e9b96e;">'hot'</span>)
plt.title(<span style="color: #e9b96e;">'Heat Map'</span>)
fig.tight_layout()
plt.savefig(<span style="color: #e9b96e;">"output_images/heatmaptest.png"</span>)
</pre>
</div>


<div class="figure">
<p><img src="output_images/heatmaptest.png" alt="heatmaptest.png" width="800px" />
</p>
</div>

<p>
Satisfied that we are acquiring the pieces we need, we are almost
ready to create our actual video-processing pipeline.  Before doing
that, however, we must address the related topics of
false-positives, buffering, smoothing, moving averages, and
time-space resolution.
</p>

<p>
In the video-processing pipeline, the raw signal will be coming at
us at a full 25 frames per second, with a lot of noise associated
with highly-transient "false-positive" detection events at locations
where there is no vehicle.  A tried-and-true way of coping with this
is to buffer the signal over some time interval (which corresponds
to some number of frames), integrate the signal over that buffer,
and substitute a (possibly weighted) moving average over the buffer
for the raw signal.  This smooths and conditions the signal,
largely eliminating false-positive detections and "jitter", though
it comes at a small price.  Our time-domain resolution dilates from
the raw frame-rate to the time interval associated with our
smoothing buffer.  Consequently, we should take care to make our
buffer "long enough, and no longer"; it should remove most false
positives, but still distinguish different cars, for instance.  
</p>

<p>
One way to implement smoothing is to introduce a bona-fide
frame-buffer, such as with a <a href="https://en.wikipedia.org/wiki/Circular_buffer">ring buffer</a> like Python's <a href="https://docs.python.org/2/library/collections.html#collections.deque"><code>deque</code></a>
data-structure.  
</p>

<p>
However, it turns out that is not at all necessary.
</p>

<p>
Instead, we can embellish the analogy we have adopted of a "heat
map" with the idea of "cooling."  First, we move the heat map data
structure (typically, a 1-channel 2-D image array of the same size
as a video frame) <i>outside</i> of the main processing loop.  A global
variable, a local variable in a <a href="https://www.programiz.com/python-programming/closure">closure</a>, or a class object's member
variable all are good candidates here.  Then, in our main processing
loop, for each video frame we deliver a "heat pulse" into the heat
map, through the <code>add_heat</code> map function above, for instance.  By
itself, that would integrate the signal over the whole video.  To
recover the notions of a short-interval buffer, a moving average,
and a smoothing kernel, we just insert a "cooling" step before the
heat pulse.  I.e., we "cool" the heat map by dis-integrating some of
its accumulated signal.  A simple way to do that is just to
down-scale it by a small <i>cooling factor</i>&#x2014;say 1%&#x2014;by multiplying
it by a fraction.
</p>

<p>
In fact, doing this imitates none other than one of the simplest and
oldest-known cooling laws in Nature, <a href="https://en.wikipedia.org/wiki/Newton's_law_of_cooling">Newton's Law of Cooling</a>.  This
is the cooling law for conductive media, and corresponds precisely
to <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential smoothing</a>
</p>

<p>
\[ \Delta T (t) = \Delta T (0) e^{-t/t_0} \]
</p>

<p>
where \(t_0\) is the cooling time-scale and relates to the cooling
factor.  Just as a guide, the following table lists the approximate
time for the heat map to cool by one-half, at the video's 25 frames
per second, for a handful of cooling factors.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="right" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="right">Cooling Factor</th>
<th scope="col" class="left">Cooling Time-Scale</th>
</tr>
</thead>
<tbody>
<tr>
<td class="right">0.99</td>
<td class="left">3 seconds</td>
</tr>

<tr>
<td class="right">0.98</td>
<td class="left">1.5 seconds</td>
</tr>

<tr>
<td class="right">0.97</td>
<td class="left">1 second</td>
</tr>

<tr>
<td class="right">0.95</td>
<td class="left">0.5 second</td>
</tr>

<tr>
<td class="right">0.90</td>
<td class="left">10 milliseconds</td>
</tr>
</tbody>
</table>

<p>
This is the buffering strategy we use here, and it works quite
well.  I decide to go for an object-oriented approach, so I made the
heat map into an object member variable.
</p>

<p>
The <code>Component</code> class defines that object, along with a host of
other attributes and methods for performing the video processing.
Hyper-parameters that govern the operation of the pipeline are
injected via the object constructor, and that includes the
<code>cooling_factor</code> described above.  Its default value is 0.98, for a
cooling time of between 1 and 2 seconds.  Note that most of the
class functions really are just thin wrappers over the functions we
developed above.  The main exception to that is the <code>get_out_img</code>
function, which composes the "scene" for the video frame.  It
supports an "extended" branch of operation wherein the main video
window is supplemented with smaller windows animating the grid
windows, and the evolving heat map.  In its layout, I made room for
5 such smaller windows, though in the end I only needed 2 of them.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">class</span> <span style="color: #8cc4ff;">Component</span>:
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">__init__</span>(<span style="color: #b4fa70;">self</span>, img,
                 cell_per_block = 4,
                 channel = 2,
                 colorspace = cv2.COLOR_RGB2HSV,
                 feature_vec = <span style="color: #e9b2e3;">True</span>,
                 orient = 9,
                 pix_per_cell = 8,
                 transform_sqrt = <span style="color: #e9b2e3;">False</span>,
                 test_size = 0.2,
                 threshold = 25,
                 numwindows = 100,
                 cooling_factor = 0.98,
                 center=<span style="color: #e9b2e3;">None</span>,
                 extended=<span style="color: #e9b2e3;">True</span>,
                 size=<span style="color: #e9b2e3;">None</span>):
        <span style="color: #b4fa70;">self</span>.bboxwindow = np.copy(image)
        <span style="color: #b4fa70;">self</span>.cell_per_block = cell_per_block
        <span style="color: #b4fa70;">self</span>.center = center <span style="color: #b4fa70;">if</span> center <span style="color: #b4fa70;">else</span> <span style="color: #e090d7;">tuple</span>(np.array(img.shape[:2][::-1])//2)
        <span style="color: #b4fa70;">self</span>.channel = channel
        <span style="color: #b4fa70;">self</span>.children = []
        <span style="color: #b4fa70;">self</span>.colorspace = colorspace
        <span style="color: #b4fa70;">self</span>.cooling_factor = cooling_factor
        <span style="color: #b4fa70;">self</span>.feature_vec = feature_vec
        <span style="color: #b4fa70;">self</span>.flat = np.zeros_like(image[:,:,0]).astype(np.<span style="color: #e090d7;">float</span>)
        <span style="color: #b4fa70;">self</span>.heatmap = np.zeros_like(image[:,:,0]).astype(np.<span style="color: #e090d7;">float</span>)
        <span style="color: #b4fa70;">self</span>.image = img
        <span style="color: #b4fa70;">self</span>.labels = <span style="color: #e9b2e3;">None</span>
        <span style="color: #b4fa70;">self</span>.mainwindow = np.copy(image)
        <span style="color: #b4fa70;">self</span>.numwindows = numwindows
        <span style="color: #b4fa70;">self</span>.orient = orient
        <span style="color: #b4fa70;">self</span>.pix_per_cell = pix_per_cell
        <span style="color: #b4fa70;">self</span>.size = size <span style="color: #b4fa70;">if</span> size <span style="color: #b4fa70;">else</span> <span style="color: #e090d7;">min</span>(img.shape[:2])//2
        <span style="color: #b4fa70;">self</span>.test_size = test_size
        <span style="color: #b4fa70;">self</span>.threshold = threshold
        <span style="color: #b4fa70;">self</span>.transform_sqrt = transform_sqrt
        <span style="color: #b4fa70;">self</span>.extended = extended


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_center</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">return</span> <span style="color: #b4fa70;">self</span>.center


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_size</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">return</span> <span style="color: #b4fa70;">self</span>.size


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">cool</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">self</span>.heatmap*=<span style="color: #b4fa70;">self</span>.cooling_factor


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_heatmap</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">return</span> <span style="color: #b4fa70;">self</span>.heatmap


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">sample</span>(<span style="color: #b4fa70;">self</span>, mainwindow, grid):
        <span style="color: #fcaf3e;">results</span> = <span style="color: #e090d7;">map</span>(process, get_patches(mainwindow, grid))
        <span style="color: #b4fa70;">return</span> results


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">heat</span>(<span style="color: #b4fa70;">self</span>, results):
        <span style="color: #fcaf3e;">samples</span> = <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> x: x[1][:2], <span style="color: #e090d7;">filter</span>(<span style="color: #b4fa70;">lambda</span> x: x[0]&gt;0, results)))
        <span style="color: #b4fa70;">for</span> s <span style="color: #b4fa70;">in</span> samples:
            <span style="color: #b4fa70;">self</span>.heatmap[s[0][1]:s[1][1],
                         s[0][0]:s[1][0]] += 1


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">evolve</span>(<span style="color: #b4fa70;">self</span>, image):
        <span style="color: #b4fa70;">self</span>.cool()
        <span style="color: #b4fa70;">self</span>.mainwindow = np.copy(image)
        <span style="color: #b4fa70;">self</span>.bboxwindow = np.copy(image)
        <span style="color: #b4fa70;">self</span>.chld_img = np.dstack([<span style="color: #b4fa70;">self</span>.flat, <span style="color: #b4fa70;">self</span>.flat, <span style="color: #b4fa70;">self</span>.flat])
        <span style="color: #fcaf3e;">grid</span> = <span style="color: #b4fa70;">self</span>.grid(<span style="color: #b4fa70;">self</span>.numwindows)
        <span style="color: #b4fa70;">self</span>.addboxes(<span style="color: #b4fa70;">self</span>.bboxwindow, grid)
        <span style="color: #fcaf3e;">results</span> = <span style="color: #b4fa70;">self</span>.sample(<span style="color: #b4fa70;">self</span>.mainwindow, grid)
        <span style="color: #b4fa70;">self</span>.heat(results)
        <span style="color: #b4fa70;">self</span>.heatmap = cv2.GaussianBlur(<span style="color: #b4fa70;">self</span>.heatmap, (31, 31), 0)
        <span style="color: #fcaf3e;">thresholded</span> = apply_threshold(<span style="color: #b4fa70;">self</span>.get_heatmap(),<span style="color: #b4fa70;">self</span>.threshold)
        <span style="color: #b4fa70;">self</span>.labels = label(thresholded)
        draw_labeled_bboxes(<span style="color: #b4fa70;">self</span>.mainwindow, <span style="color: #b4fa70;">self</span>.labels)


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">get_out_img</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">if</span> <span style="color: #b4fa70;">self</span>.extended:
            <span style="color: #fcaf3e;">bbox_img</span> = cv2.resize(<span style="color: #b4fa70;">self</span>.bboxwindow, <span style="color: #e090d7;">tuple</span>(np.array(<span style="color: #b4fa70;">self</span>.image.shape[:2][::-1])//2))
            <span style="color: #fcaf3e;">hot2_img</span> = cv2.resize(scale(np.dstack([<span style="color: #b4fa70;">self</span>.get_heatmap(), <span style="color: #b4fa70;">self</span>.get_heatmap(), <span style="color: #b4fa70;">self</span>.flat]), 2*<span style="color: #b4fa70;">self</span>.threshold), <span style="color: #e090d7;">tuple</span>(np.array(image.shape[:2][::-1])//2))
            cv2.putText(hot2_img, <span style="color: #e9b96e;">"Max: %.2f"</span> % np.<span style="color: #e090d7;">max</span>(<span style="color: #b4fa70;">self</span>.get_heatmap()), (25,25), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2)
            cv2.putText(hot2_img, <span style="color: #e9b96e;">"Threshold: %.2f"</span> % np.<span style="color: #e090d7;">max</span>(<span style="color: #b4fa70;">self</span>.threshold), (25,55), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2)
            cv2.putText(hot2_img, <span style="color: #e9b96e;">"Cooling Fac.: %.2f"</span> % np.<span style="color: #e090d7;">max</span>(<span style="color: #b4fa70;">self</span>.cooling_factor), (25,85), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2)
            <span style="color: #fcaf3e;">flat_img</span> = cv2.resize(np.dstack([<span style="color: #b4fa70;">self</span>.flat, <span style="color: #b4fa70;">self</span>.flat, <span style="color: #b4fa70;">self</span>.flat]), <span style="color: #e090d7;">tuple</span>(np.array(image.shape[:2][::-1])//2))
            <span style="color: #fcaf3e;">outp_img</span> = cv2.resize(np.hstack((np.vstack((<span style="color: #b4fa70;">self</span>.mainwindow,
                                                        np.hstack((flat_img,
                                                                   flat_img)))),
                                             np.vstack((bbox_img,
                                                        hot2_img,
                                                        flat_img)))),
                                  <span style="color: #e090d7;">tuple</span>(np.array(<span style="color: #b4fa70;">self</span>.image.shape[:2][::-1])))
        <span style="color: #b4fa70;">else</span>:
            <span style="color: #fcaf3e;">outp_img</span> = <span style="color: #b4fa70;">self</span>.mainwindow
        <span style="color: #b4fa70;">return</span> outp_img


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">grid</span>(<span style="color: #b4fa70;">self</span>, num):
        <span style="color: #b4fa70;">return</span> <span style="color: #e090d7;">list</span>(random_scan4(<span style="color: #b4fa70;">self</span>.image, 2, num,width=60,left=-30))
        <span style="color: #b4fa70;">return</span> <span style="color: #e090d7;">list</span>(random_scan5(<span style="color: #b4fa70;">self</span>.image, 2, num))


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">addboxes</span>(<span style="color: #b4fa70;">self</span>, bboxwindow, grid):
        <span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #b4fa70;">lambda</span> w: draw_window(bboxwindow, w[:2]), grid))


    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">process_image</span>(<span style="color: #b4fa70;">self</span>, image):
        <span style="color: #b4fa70;">self</span>.evolve(image)
        <span style="color: #b4fa70;">return</span> <span style="color: #b4fa70;">self</span>.get_out_img()
</pre>
</div>

<p>
First, we use the <code>Component</code> class to process the test video.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">in_clip</span> = VideoFileClip(<span style="color: #e9b96e;">"test_video.mp4"</span>)
<span style="color: #fcaf3e;">scene</span> = Component(scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>)),threshold=20,cooling_factor=0.99)
<span style="color: #fcaf3e;">out_clip</span> = in_clip.fl_image(scene.process_image)
out_clip.write_videofile(<span style="color: #e9b96e;">"output_images/test_output.mp4"</span>, audio=<span style="color: #e9b2e3;">False</span>)
</pre>
</div>

<p>
The test video is very short, so there barely is enough time for
"heat to build up" to the point that one car is detected, let alone
two.  
</p>

<iframe width="800" height="450" src="https://www.youtube.com/embed/McviDE-LWLA" frameborder="0" allowfullscreen></iframe>

<p>
Next, we process the main project video in "extended mode", to
include the smaller sub-windows the the animated grid windows and
evolving heat map.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">in_clip</span> = VideoFileClip(<span style="color: #e9b96e;">"project_video.mp4"</span>)
<span style="color: #fcaf3e;">scene</span> = Component(scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>)),threshold=40,cooling_factor=0.99)
<span style="color: #fcaf3e;">out_clip</span> = in_clip.fl_image(scene.process_image)
out_clip.write_videofile(<span style="color: #e9b96e;">"output_images/project_output_extended.mp4"</span>, audio=<span style="color: #e9b2e3;">False</span>)
</pre>
</div>

<p>
As this video is in "extended mode", you can see both the animated
grid windows, and the evolving heat map.  Note that here, we
actually have moved away from <code>image_plane_scan</code> for generating the
grid windows, and in fact are using <code>random_scan4</code> which, the reader
will recall, lays out grids randomly in "physical space" in a volume
that corresponds to a long, thick "ribbon".  Here, the ribbon
extends across the road and from the road up to camera height.  The
randomness of the windows should be self-evident from the video.
</p>

<p>
There still are a few transient false-positives at a few places,
unfortunately.  More tuning of the cooling, thresholding, and other
parameters might banish them.  
</p>

<iframe width="800" height="450" src="https://www.youtube.com/embed/yGHN0OlBVRU" frameborder="0" allowfullscreen></iframe>

<p>
Finally, we repeat processing for the main project video, but not in
extended mode.  This mainly was so that I could get a sense of the
difference in processing times.  In fact, it cuts the processing
time by about a half, indicating that generating, resizing, and
composing together all the windows introduces considerable
overhead.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">in_clip</span> = VideoFileClip(<span style="color: #e9b96e;">"project_video.mp4"</span>)
<span style="color: #fcaf3e;">scene</span> = Component(scale(mpimg.imread(<span style="color: #e9b96e;">"test_images/test1.jpg"</span>)),threshold=40,cooling_factor=0.99,extended=<span style="color: #e9b2e3;">False</span>)
<span style="color: #fcaf3e;">out_clip</span> = in_clip.fl_image(scene.process_image)
out_clip.write_videofile(<span style="color: #e9b96e;">"output_images/project_output.mp4"</span>, audio=<span style="color: #e9b2e3;">False</span>)
</pre>
</div>

<p>
This video is largely the same as the previous one, save for the
fact that the smaller sub-windows are not present
</p>

<iframe width="800" height="450" src="https://www.youtube.com/embed/I6_oEk4CKBs" frameborder="0" allowfullscreen></iframe>
</div>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7">Discussion</h2>
<div class="outline-text-2" id="text-7">
<p>
I found this to be a very stimulating, but also a very challenging
project.  I believe it would have been much easier had I used any of
the code that Udacity provided in the lectures, however I rejected
that almost completely.  But, that opened up the possibility to try
other interesting techniques.  
</p>

<p>
To recap, some of the alternative techniques I enjoyed adding were
these.  
</p>

<ul class="org-ul">
<li>Use dead-simple feature extraction based on just 1-channel HOG
extraction.
</li>
<li>Flip the training images horizontally to essentially double the
training set size.
</li>
<li>Tried modeling sliding windows, and estimating vehicle
detections, in 3-D real-world physical coordinates.  Though I
never got to the last step, I re-used much of the 3-D
projection/deprojection code, and it added additional insight.
</li>
<li>Used random windows rather than strictly overlapping windows laid
out on a regular array.
</li>
<li>Re-used the 3-D projection/deprojection code so as to scale the
window sizes automatically with distance.
</li>
<li>Borrowed from Nature in order to implement exponential
moving-average buffering of the heat map, without ever having to
buffer any actual heat map frames.
</li>
<li>Adorned the main video output window with animations of the grid
windows and of the evolving heat map, which aided in parameter
tuning and added additional insight.

<p>
Of course, I also believe I met the main objectives for the
project.  
</p>
</li>

<li>Performed a Histogram of Oriented Gradients (HOG) feature
extraction on a labeled training set of images and trained a
classifier.
</li>

<li>Normalized the features and randomized the selection for training
and testing.
</li>

<li>Implemented a sliding-window technique and used the trained
classifier to search for vehicles in images.
</li>

<li>Ran a pipeline on video streams and created a heat map of
recurring detections frame by frame to reject outliers and follow
detected vehicles.
</li>

<li>Estimated a bounding box for detected vehicles.
</li>
</ul>
</div>

<div id="outline-container-sec-7-1" class="outline-3">
<h3 id="sec-7-1">Rubric Points</h3>
<div class="outline-text-3" id="text-7-1">
</div><div id="outline-container-sec-7-1-1" class="outline-4">
<h4 id="sec-7-1-1"><span class="done DONE">DONE</span> Explain how (and identify where in your code) you extracted HOG features from the training images.</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
This is described above in the section <b>Histogram of Oriented Gradients (HOG)</b>
</p>
</div>
</div>

<div id="outline-container-sec-7-1-2" class="outline-4">
<h4 id="sec-7-1-2"><span class="done DONE">DONE</span> Explain how you settled on your final choice of HOG parameters.</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
This also is described above.  To recap, I started with the HOG
parameters we used in the quizzes in the lecture, and then
manually tuned them from there.  Not much tuning was needed.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-3" class="outline-4">
<h4 id="sec-7-1-3"><span class="done DONE">DONE</span> Describe how (and identify where in your code) you trained a classifier using your selected HOG features</h4>
<div class="outline-text-4" id="text-7-1-3">
<p>
This is described above in the section <b>Histogram of Oriented Gradients (HOG)</b>
</p>
</div>
</div>

<div id="outline-container-sec-7-1-4" class="outline-4">
<h4 id="sec-7-1-4"><span class="done DONE">DONE</span> Describe how (and identify where in your code) you implemented a sliding window search.</h4>
<div class="outline-text-4" id="text-7-1-4">
<p>
This is described above in the sections <b>Sliding Window Search I</b> and <b>Sliding Window Search II</b>.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-5" class="outline-4">
<h4 id="sec-7-1-5"><span class="done DONE">DONE</span> How did you decide what scales to search and how much to overlap windows?</h4>
<div class="outline-text-4" id="text-7-1-5">
<p>
This is described above in the sliding windows sections.  To
recap, in the end since I modeled the windows in 3-D real world
physical coordinates before projecting them onto the image
plane, the sliding windows automatically scaled with distance. 
</p>
</div>
</div>

<div id="outline-container-sec-7-1-6" class="outline-4">
<h4 id="sec-7-1-6"><span class="done DONE">DONE</span> Show some examples of test images to demonstrate how your pipeline is working.</h4>
<div class="outline-text-4" id="text-7-1-6">
<p>
Example test images of the heat map and detected vehicles appear
above in the <b>Video Implementation</b>.  Beyond that, insight into
how the pipeline is working can be gleaned by the animated grid
windows and evolving heat map sub-windows in the 2 of the 3
processed videos.  
</p>
</div>
</div>

<div id="outline-container-sec-7-1-7" class="outline-4">
<h4 id="sec-7-1-7"><span class="done DONE">DONE</span> How did you optimize the performance of your classifier?</h4>
<div class="outline-text-4" id="text-7-1-7">
<p>
I did not spend a great deal of time trying to optimize the
performance of the classifier.  Largely, my steps toward
optimization the whole pipeline comprised these things.
</p>

<ol class="org-ol">
<li>Stuck to a ruthlessly simple classifier:  1-channel HOG
feature extraction only.
</li>
<li>Reduced the total number of sliding windows.  Using random
windows helped with this a great deal.
</li>
<li>(Optionally) remove the sub-windows during video processing.
</li>
</ol>

<p>
Even still, the processing does not work in real-time.  In order
for that to occur, I might have to skip some frames, add in some
multiprocessing features, or try more advance techniques.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-8" class="outline-4">
<h4 id="sec-7-1-8"><span class="done DONE">DONE</span> Provide a link to your final video output.</h4>
<div class="outline-text-4" id="text-7-1-8">
<p>
Obviously, this is done above. 
</p>
</div>
</div>

<div id="outline-container-sec-7-1-9" class="outline-4">
<h4 id="sec-7-1-9"><span class="done DONE">DONE</span> Your pipeline should perform reasonably well on the entire project video</h4>
<div class="outline-text-4" id="text-7-1-9">
<p>
I have somewhat wobbly and slightly unstable bounding boxes but
am identifying the vehicles most of the time with minimal false
positives.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-10" class="outline-4">
<h4 id="sec-7-1-10"><span class="done DONE">DONE</span> Describe how (and identify where in your code) you implemented some kind of filter for false positives</h4>
<div class="outline-text-4" id="text-7-1-10">
<p>
This is described above in the <b>Video Implementation</b> section.
To recap, I "cool" the heat map between frames simply by
multiplying it by a fraction close to 1 (~0.99).  This provides
a nice, exponential moving average.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-11" class="outline-4">
<h4 id="sec-7-1-11"><span class="done DONE">DONE</span> Describe how (and identify where in your code) you implemented some kind of method for combining overlapping bounding boxes.</h4>
<div class="outline-text-4" id="text-7-1-11">
<p>
Combining or "overlapping" windows occurs by virtue of two
factors:  random window placement, and integrating multiple
frames into a heat map.  Random window placement means that even
with a small overall number of windows, the likelihood that
windows will overlap is very high, especially integrating over
multiple frames, and that's exactly what we do with the heat
map.  
</p>
</div>
</div>

<div id="outline-container-sec-7-1-12" class="outline-4">
<h4 id="sec-7-1-12"><span class="done DONE">DONE</span> Briefly discuss any problems / issues you faced in your implementation of this project.</h4>
<div class="outline-text-4" id="text-7-1-12">
<p>
The main problem I had was that my ambitious goal of identifying
vehicles in 3D proved too challenging given the time allotted.
Beyond that, I paid a price in not optimizing the video
processing performance, in that it made parameter tuning a slow
and tedious process.
</p>

<p>
Another problem I notice is that, despite the good performance
of the classifier on the test set, in the video it seems it
succeeds in identifying cars largely based on the tail-lights on
the back of the cars.  This a problem insofar as when cars enter
the stage, the classifier is relatively unaware of it until the
tail of the car enters the stage as well.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-13" class="outline-4">
<h4 id="sec-7-1-13"><span class="done DONE">DONE</span> Where will your pipeline likely fail?</h4>
<div class="outline-text-4" id="text-7-1-13">
<p>
The pipeline likely will fail under challenging lighting
conditions because I did not attempt to make it more robust
along that dimension.  And, because I did not optimize
aggressively for performance, it probably would be impractical
at much higher frame rates.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-14" class="outline-4">
<h4 id="sec-7-1-14"><span class="done DONE">DONE</span> What could you do to make it more robust?</h4>
<div class="outline-text-4" id="text-7-1-14">
<p>
There are many things I could do to make it more robust.  
</p>

<ol class="org-ol">
<li>Back off from a simple classifier, and add in color
histograms, spatial binning, etc.
</li>
<li>Perhaps switch to a Convolutional Neural Network classifier.
</li>
<li>Aggressively optimize performance.  This would make it more
robust insofar as it would make aggressive hyper-parameter
optimization more feasible.
</li>
<li>The thresholding/labeling approach to identifying vehicles is
not very robust.  SciKit has a <a href="http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_blob.html">variety</a> of "Blob Detection"
tools, with various performance characteristics.
</li>
<li>With better vehicle detection, instead of applying a sliding
window search agnostically, it could be done just in the
neighborhood of known cars.  I actually started heading down
this path, and this is the main reason I adopted an
object-oriented approach.  Alas, time pressures forced me to
abandon it.
</li>
</ol>

<p>
As you can see, there is much room for improvement.
Nevertheless, I am happy with my solution, and pleased that I
discovered a few new (to me) techniques along the way.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: David A. Ventimiglia (<a href="mailto:dventimi@gmail.com">dventimi@gmail.com</a>)</p>
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-03-08&gt;</span></span></p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
